\documentclass[12pt, twoside]{article}
\usepackage{jmlda}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage[english,russian]{babel}

\newcommand{\hdir}{.}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\baselinestretch}{1.8} 

\begin{document}

% Здесь можно определять собственные команды, они будут действовать только внутри статьи:
\newenvironment{coderes}%
    {\medskip\tabcolsep=0pt\begin{tabular}{>{\small}l@{\quad}|@{\quad}l}}%
    {\end{tabular}\medskip}


\title{Метрический анализ пространства параметров глубоких нейросетей}
\author{Эрнест Р.~Насыров}
\email{nasyrov.rr@phystech.edu}
%organization{ФИЦ <<Информатика и управление>> РАН, г.~Москва, ул.~Вавилова, 44/2}
\abstract{
Исследуется проблема снижения размерности пространства параметров модели машинного обучения. Решается задача восстановления временного ряда. Для восстановления используются авторегресионные модели: линейные, автоенкодеры, реккурентные сети - с непрерывным и дискретным временем. Проводится метрический анализ пространства параметров модели.  Предполагается, что отдельные параметры модели, случайные величины, собираются в векторы, многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования нашей работы.  Этот анализ снижает число параметров модели, оценивает значимости параметров, отбирая их. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов \textit{бутстрэпа} и \textit{вариационного вывода}. Эксперименты проводятся на задачах восстановления синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных. Для восстановления применяются модели SSA, нелинейного PCA, RNN, Neural ODE.
}
\titleEng{Style guide for authors}
\authorEng{JMLDA editorial board}
\organizationEng{Federal Research Center ``Computer Science and Control'' of RAS, 44/2~Vavilova~st., Moscow, Russia}
\abstractEng{
    This document explains how to prepare papers using \LaTeXe\ typesetting system and \texttt{jmlda.sty} package.
}
%\doi{10.21469/22233792}
%\receivedRus{01.01.2017}
%\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\title{Метрический анализ пространства параметров глубоких нейросетей.}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{Эрнест Р.~Насыров \\
	\texttt{nasyrov.rr@phystech.edu} \\
	%% examples of more authors
    }
	
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\


% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}

%\renewcommand{\baselinestretch}{1.8} 

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Метрический анализ пространства параметров глубоких нейросетей}
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Эрнест Р.~Насыров},
pdfkeywords={нет слова},
}



% Старый Abstract
\begin{comment}
В работе исследуется проблема снижения размерности пространства параметров модели. В ее рамках решается задача восстановления временного ряда. В качестве модели восстановления ряда используются различные автоенкодеры. В работе проводится метрический анализ пространства параметров автоенкодера. Новизна заключается в том, что отдельные параметры модели - случайные величины - собираются в векторы – многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования нашей работы. Этот анализ позволит снизить количество параметров модели, сделать выводы о значимости параметров, произвести их отбор. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов bootstrap и variational inference. Эксперименты проводятся на моделях SSA, RNN и VAE на задачах предсказания синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных.
\end{comment}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}

% 1. История + связанные понятия (выбор оптимально модели ...).




%%% С развитием технологий скорость обработки данных растет, они становятся более сложными, большей размерности. 
%%% Привести цифры (из Nature), а пока коммент

%%% ССЫЛКА НА НЕДАВНИЕ РАБОТЫ ПО ТЕМЕ

Высокоразмерные данные избыточны, что представляет сложность для их эффективной обработки и использования. В работе решается задача снижения размерности признакового описания объекта.
%%%, которая привлекла большое внимание ученых.
%%% ССЫЛКИ НА СТАТЬИ/УЧЕНЫХ
Ее базовый принцип состоит в том, чтобы отобразить высокоразмерное признаковое пространство в низкоразмерное, сохраняя важную информацию о данных \citep{jia2022feature}.

На текущий момент известно много методов снижения размерности данных. В работе \citep{ornek2019nonlinear} снижения размерности достигается за счет построения дифференцируемой функции эмбеддинга в низкоразмерное представление, а в \citep{cunningham2014dimensionality} обсуждаюся линейные методы. В работе \citep{isachenko2022quadratic} задача снижения размерности решается для предсказания движения конечностей человека по электрокортикограмме с использованием  метода QPFS, учитывающем мультикоррелированность и входных, и целевых признаков.

Наряду с задачей снижения размерности входных данных стоит задача выбора оптимальной структуры модели. В случае оптимизации структуры нейросети, большое внимание уделено изучению признакового пространства модели. В работах \citep{hassibi1993optimal} и \citep{dong2017learning} применяется метод OBS (Optimal Brain Surgeon), состоящий в удалении весов сети с сохранением ее качества аппроксимации, причем выбор удаляемых весов производится с помощью вычисления гессиана функции ошибки по весам.

В статье \citep{грабовой2019определение} приводится метод первого порядка, решающий задачу удаления весов, основанный на нахождении дисперсии градиента функции ошибки по параметру и анализе ковариационной матрицы параметров, а в статье \citep{грабовой2020введение} нерелевантные веса не удаляют, а прекращают их обучение.


% 2. А вот тут пошла актуальность
Приведенные выше задачи понижения размерности данных и выбора оптимальной структуры нейросети основаны на исследовании пространства входных данных и пространства признаков соответственно. На взгляд авторов статьи, существенный недостаткок предыдущих работ состоит в том, что в них анализируются \textit{отдельные} параметры (скаляры) моделей и их взаимозависимость. Тем самым не учитывается, что на входные данные действуют \textit{вектора} параметров посредством скалярных произведений, то есть упускается из виду простая \textit{структура} преобразования.

В данной работе мы решаем задачу восстановления временного ряда, в рамках которой занимаемся проблемой снижения пространства параметров модели, основанном на анализе сопряженного пространства ко входному, которое связывает входное пространство и пространство параметров. 

Наше исследование в большой степени полагается на простоту устройства глубоких нейросетей, которые являются композицией линейных и простых нелинейных функций (функций активации). Составной блок нейросети описан формой:  $y=\sigma(\mathsf{W}x), y \in \mathbb{R}^m, x \in \mathbb{R}^n, \mathsf{W} \in \mathbb{R}^{m \times n}, \sigma: \mathbb{R} \to \mathbb{R}$. Раньше элементы $\mathsf{W}_{ij}$ исследовались по-отдельности, как скаляры. Авторы работы предлагают изучать их как векторы-строки $\mathbf{w}_1, \dots, \mathbf{w}_m: \mathsf{W} = \begin{pmatrix}
\mathbf{w}_1^{\mathsf{T}}\\
\dots\\
\mathbf{w}_m^{\mathsf{T}}\\
\end{pmatrix}$. В нейросети эти строки обычно называются \textit{нейронами}. В SSA $\sigma = Id$, а матрица $\mathsf{W}=\mathsf{W}_k$ это приближение истинной матрицы фазовых траекторий $\mathsf{X}$ (матрицы Ганкеля) суммой $k$ элементарных матриц.

% Объяснение фазовой траектории

Пусть $\mathbf{x} = [x_1, \dots, x_N]^{\mathsf{T}}, x_i \in \mathbb{R}$ - временной ряд, $1 \le n \le N$ - ширина окна. Точка $\mathbf{x}_t = [x_t, \dots, x_{t + n - 1}]^{\mathsf{T}}$ является точкой фазовой траектории временного ряда в траекторном пространстве $\mathbb{H}_{\mathbf{x}} \subset \mathbb{R}^n$. 

Предполагается, что каждая точка фазовой траектории распределена нормально вокруг своего матожидания. Тогда и временной ряд является случайным, поэтому результат обучения модели на нем, то есть параметры обученной модели, будут случайными.

%Тогда обучающая выборка $\mathfrak{D}=(\mathbf{X}, \mathbf{y})$ это набор случайных величин, поэтому и результат обучения модели на ней, то есть веса модели тоже будут случайными. 

В работе исследуется положение случайных векторов параметров модели $\mathbf{w}_i$ в метрическом пространстве. С помощью методов бутстрэпа и вариационного вывода \citep{hastie2009elements} оцениваются их матожидания $\mathbf{e}_i=E(\mathbf{w}_i)$ и ковариационные матрицы $D(\mathbf{w}_i) = \mathbf{A}^{-1}_i$. Мы работаем в гипотезе, что эти векторы $\mathbf{w}_i$ распределены нормально, таким образом пара $(\mathbf{e}_i, \mathbf{A}^{-1}_i)$ полностью описывает вероятностное распределение вектора $\mathbf{w}_i$. 

\begin{wrapfigure}{r}{0.30\textwidth}

\includegraphics[width=0.25\textwidth]{gaussian_mixture.jpg}
\caption{Смесь гауссианов трех 2-х мерных векторов.}
\label{ris:gauss_mixture}

\end{wrapfigure}
В качестве графического анализа пространства изображаются положения этих векторов как смеси гауссианов \ref{ris:gauss_mixture}.  На рисунке изображены плотности функции распределения трех гауссовских векторов (вертикальная ось) в зависимости от их положения на плоскости. В каждой точке плотность равна сумме плотностей трех распределений, отнормированная таким образом, чтобы площадь под графиком равнялась $1$. Центрам 'куполов' соответствуют матожидания векторов, а их форма определяется матрицей ковариации. Таким образом, чем ниже и шире 'купол', тем больше дисперсия и наоборот.

\begin{wrapfigure}{l}{0.30\textwidth}
\includegraphics[width=0.25\textwidth]{gaussian_conf_area.jpg}
\caption{Доверительные области 3-х мерных векторов.}
\label{ris:gauss_conf_area}
\end{wrapfigure}
Также изображается $95\%$ доверительная область каждого вектора \ref{ris:gauss_conf_area}. На картинке изображены эллипсы, соответствующие доверительным областям для гауссовских векторов в 3-х мерном пространстве. Чем больше ширина эллипса вдоль направления, тем больше дисперсия вектора по этому направлению.

% Картинки, иллюстрирующие сказанное выше

Уменьшение размерности достигается за счет метрического анализа пространства векторов-параметров путем отбора релевантных строк (с малой дисперсией), замены мультикоррелирующих строк на их линейную композицию с помощью обобщения алгоритма QPFS, изучения структуры сообществ строк.

В качестве базовых моделей используются SSA (\citep{golyandina2001analysis}), нелинейный PCA, RNN (\citep{bronstein2021geometric}), VAE (\citep{kingma2019introduction}) и Neural ODE (\citep{chen2018neural}).

Задача восстановления временного ряда решается на синтетических данных зашумленного $sin$, данных показания акселерометра в датасете MotionSense3 \citep{malekzadeh2018protecting}, периодичных видеоданных. 

\section{Problem statement}
Пусть имеется множество из $m$ временных рядов $\mathcal{S}=\{\mathbf{s}_1, \dots, \mathbf{s}_{m}\}, \mathbf{s}_i = [\mathbf{s}_i^1, \dots, \mathbf{s}_i^T], \mathbf{s}_i^j \in \mathbb{R}$, где $n$ - длина сигналов. Каждый временной ряд - последовательность измерений величины в течение времени.

\begin{definition}
Временное представление $\mathbf{x}_t = [\mathbf{s}_1^t, \dots, \mathbf{s}_m^t]^{\mathsf{T}} \in \mathbb{R}^m$ состоит из измерений временных рядов в момент времени $t$.
\end{definition}


\begin{definition}
Представление предыстории длины $h$ для момента времени $t$ множества временных рядов $\mathcal{S}$ - это матрица $\mathbf{X}_{t,h} = [\mathbf{x}_{t-h+1}, \dots, \mathbf{x}_{t}]^{\mathsf{T}} \in \mathbb{R}^{h \times m}$.
\end{definition}

\begin{definition}
Представление горизонта прогнозирования длины $p$ для момента времени $t$ множества временных рядов $\mathcal{S}$ - это матрица $\mathbf{Y}_{t, p} = [\mathbf{x}_{t+1}, \dots, \mathbf{x}_{t+p}]^{\mathsf{T}}$.
\end{definition}

\begin{definition}
Прогностическая модель $\mathbf{f}^{\mathsf{AR}}: \mathbb{R}^{h \times m} \to \mathbb{R}^{p \times m}$ является авторегрессионной моделью, которая по представлению предыстории $\mathbf{X}_{t, h}$ предсказывает представление горизонта планирования $\mathbf{Y}_{t, p}$.
\end{definition}

Решается задача авторегрессионного декодирования. Она состоит в построении прогностической модели $\mathbf{f}^{\mathsf{AR}}$, дающий \textit{представление горизонта прогнозирования} множества временных рядов по \textit{представлению предыстории} того же множества рядов. В дальнейшем будем считать, что восстанавливаем 1 временной ряд, что есть что $m=1$.

Обозачим множество всех одномерных временных рядов через $\mathbb{S}: \mathbb{S} = \bigcup\limits_{n=1}^{+\infty}\{[s_1, \dots, s_n] \in \mathbb{R}^{n}\}$. Тогда прогностическая модель это функция $f^{\mathsf{AR}}: \mathbb{S} \to \mathbb{R}$. Изначальный временной ряд $\mathbf{s} = [s_1, \dots, s_T]$ делится на две части $\mathbf{s} = [\mathbf{s}^H|\mathbf{s}^T], \mathbf{s}^H = [s_1, \dots, s_h], \mathbf{s}^T = [s_{h+1}, \dots, s_T]$. Задача состоит в том, чтобы предсказать $\mathbf{s}^T$ с максимальной точностью. Предсказание происходит следующим образом:

\begin{enumerate}
    \item С помощью модели $f$ предсказывается $\hat{s}_{h+1}$.
    \item Предсказанный элемент $\hat{s}_{h+1}$ вместе с исходным временным рядом $\mathbf{s}^H$ подаются на вход $f$ для предсказания $\hat{s}_{h+2}$.
    \item Шаги $1-2$ повторяются, пока не будет предсказан весь $\hat{\mathbf{s}}^T=[\hat{s}_{h+1}, \dots, \hat{s}_{T}]$.
\end{enumerate}

Для упрощения нотации обозначим $f(\mathbf{s}^H)=\hat{\mathbf{s}}^T = [\hat{s}_{t+1}, \dots, \hat{s}_{T}]$.

Модель $f = f(\mathbf{w}, \mathbf{s}), \mathbf{w} \in \mathbb{W}, \mathbf{s}=[s_1, \dots, s_t]\in \mathbb{R}^t$ выбирается из некоего параметрического семейства.
Параметры модели выбираются таким образом, чтобы минимизировать функцию ошибки $S=S(\mathbf{w}|\mathbf{s},f)$:
$$
\mathbf{w^*} = \argmin\limits_{\mathbf{w} \in \mathbb{W}} S(\mathbf{w}|\mathbf{s},f).
$$


В работе будет использоваться функция ошибки MSE, то есть 
$$
S(\mathbf{w}|\mathbf{s},f) = \sum\limits_{t=h+1}^{T}(\mathbf{s}_t - \hat{\mathbf{s}}_t)^2.
$$.


В качестве моделей для восстановления временного ряда будем использовать SSA, двуслойную нейросеть с ортогональными линейными преобразованиями, RNN и Neural ODE. Остановимся на каждой из них подробнее.

В модели SSA восстановление временного ряда получается за счет разложения матрицы фазовой траектории в сумму одноранговых матриц. По временному ряду $\mathbf{s} = [s_1, \dots, s_{T}]$ и ширине окна $h$ строится матрица фазовых траекторий 

$\mathbf{X} = \begin{pmatrix}
s_1 & \dots & s_{k}\\
s_2 & \dots & s_{k+1}\\
\dots & \dots & \dots\\
s_{h} & \dots & s_{T}\\
\end{pmatrix} = \big[X_1 : \dots : X_k\big], k=T-h+1, X_i = [s_i, s_{i+1}, \dots, s_{i + h - 1}] \in \mathbb{R}^h$, которая является матрицей Ганкеля, так как каждая диагональ вида $i+j=const$ содержит одинаковые элементы. Векторы $X_i$ являются точками фазовой траектории сигнала. Далее вычисляются собственные значения и соответствующие собственным подпространствам ортонормированные системы векторов матрицы $\mathbf{S}=XX^T \in \mathbb{R}^{h \times h}$. Пусть $\lambda_1 \ge \dots \ge \lambda_L \ge 0$ - собственные значения $\mathbf{S}$, $u_1, \dots u_h$ - ортонормированная система собственных векторов, соответствующая собственным значениям, $v_i = \frac{\mathbf{X}^Tu_i}{\sqrt{\lambda_i}} (i=1, \dots, h)$. Тогда $\mathbf{X} = \mathbf{X}_1 + \dots + \mathbf{X}_d, \mathbf{X}_i = \sqrt{\lambda_i}u_iv_i^T$ - SVD-разложение $\mathbf{X}$. Далее происходит группировка матриц $\mathbf{X}_i$, их ганкелизация и восстанавливается матрица сигнала $\hat{\mathbf{X}} = \mathbf{\tilde{X}}_{I_1} + \dots + \mathbf{\tilde{X}}_{I_m}, I_1 \sqcup \dots \sqcup I_m \subset \{1, \dots, h\}, \mathbf{\tilde{X}}_{I} = hank\big(\sum\limits_{i \in I}\mathbf{X}_i\big)$. Где $hank(X)$ - ганкелизация матрицы $X$, состоящая в том, что на каждой диагонали вида $i+j=const$ все элементы заменяются на их среднее арифметическое. Параметрами модели SSA являются множества $I_1, \dots, I_m$.


Модель двуслойной нейросети задается следующим образом: $f(x) = \sigma(w\cdot \sigma(Wx)), x \in \mathbb{R}^{h}, W \in \mathbb{R}^{d \times h}, w \in \mathbb{R}^d: w^Tw=1, W^TW=I$. Последние два условия гарантируют, что преобразования будет ортогональным. Здесь нейросеть предсказывает значение сигнала в следующий момент времени на основе его значения в предыдущие $t$ моментов, где $t$ - ширина окна. Нелинейность обеспечивается выбором функции активации $\sigma(x)$.

Модель RNN задается следующим образом: \\
$
h_t = \sigma(W \cdot h_{t-1} + V \cdot \mathbf{x}_t)$
$\\
s_{t+1} = tanh(w_o^{\mathsf{T}} \cdot h_t)
$,\\ где $h_{t} \in \mathbb{R}^{d}$ - скрытое состояние RNN в момент времени $t$, $W \in \mathbb{R}^{d \times d}, V \in \mathbb{R}^{d \times h}. \\
\mathbf{x}_t = [s_{t - h + 1}, \dots, s_{t}] \in \mathbb{R}^h$ - временной ряд, подающийся на вход модели, $w_o \in \mathbb{R}^{d}, s_{t+1} \in \mathbb{R}$ - прогноз значения сигнала в момент времени $t+1$. Параметрами модели являются матрицы $W, V, w_0$ а также начальное скрытое состояние $h_0$, которое мы зафиксируем $h_0=0$.

О входном временном ряде выдвинута гипотеза о том, что точки на фазовой траектории распределены по нормальному закону, то есть что $\mathbf{x}_t = [s_t, \dots, s_{t + h - 1}] \sim \mathcal{N}(\hat{\mathbf{x}}_t, \mathbf{B})$, где $\hat{\mathbf{x}}_t$ - это матожидание точки фазовой траектории в момент времени $t$, а $\mathbf{B}$ - матрица ковариации. Мы также предполагаем, что $\mathbf{w} \sim \mathcal{N}(\hat{\mathbf{w}}, \mathbf{A})$. 

В работе оцениваются матожидание вектора параметров модели $\hat{\mathbf{w}}$, а также матрица ковариации параметров $\mathbf{A}$ с помощью методов бутстрепа и вариационного вывода.

Для метрического анализа пространства параметров выбираются набор множеств индексов $\mathcal{I}_1 \sqcup \dots \sqcup \mathcal{I}_k \subset \{1, \dots, len(\mathbf{w})\}$, рассматриваются соответствующие им подвектора параметров $\mathbf{w}_{\mathcal{I}_1}, \dots, \mathbf{w}_{\mathcal{I}_k}$
Считается, что каждый $\mathbf{w}_{\mathcal{I}_j}$ соответствует <<смысловой единице>> модели. В нейросетевых моделях это строки матрицы линейного преобразования пространства параметров $W$, которые обычно называют \textit{нейронами}.


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .





\end{document}
