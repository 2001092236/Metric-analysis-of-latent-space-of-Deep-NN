\documentclass[12pt, twoside]{article}
\usepackage{jmlda}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage[english,russian]{babel}

\newcommand{\hdir}{.}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\baselinestretch}{1.8} 

\begin{document}

% Здесь можно определять собственные команды, они будут действовать только внутри статьи:
\newenvironment{coderes}%
    {\medskip\tabcolsep=0pt\begin{tabular}{>{\small}l@{\quad}|@{\quad}l}}%
    {\end{tabular}\medskip}


\title{Метрический анализ пространства параметров глубоких нейросетей}
\author{Эрнест Р.~Насыров}
\email{nasyrov.rr@phystech.edu}
%organization{ФИЦ <<Информатика и управление>> РАН, г.~Москва, ул.~Вавилова, 44/2}
\abstract{
Исследуется проблема снижения размерности пространства параметров модели машинного обучения. Решается задача восстановления временного ряда. Для восстановления используются авторегресионные модели: линейные, автоенкодеры, реккурентные сети ~--- с непрерывным и дискретным временем. Проводится метрический анализ пространства параметров модели.  Предполагается, что отдельные параметры модели, случайные величины, собираются в векторы, многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования данной работы.  Этот анализ снижает число параметров модели, оценивает значимости параметров, отбирая их. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов \textit{бутстрэпа} и \textit{вариационного вывода}. Эксперименты проводятся на задачах восстановления синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных. Для восстановления применяются модели SSA (singular spectrum analysis), нелинейного PCA (principal component analysis), RNN (reccurent neural network), Neural ODE (neural ordinary differential equations).

\bigskip
\noindent
\textbf{Ключевые слова}: \emph {Временные ряды; снижение размерности; релевантность параметров; пространство параметров; выбор модели.}
}

\titleEng{Style guide for authors}
\authorEng{JMLDA editorial board}
\organizationEng{Federal Research Center ``Computer Science and Control'' of RAS, 44/2~Vavilova~st., Moscow, Russia}
\abstractEng{
    This document explains how to prepare papers using \LaTeXe\ typesetting system and \texttt{jmlda.sty} package.
}
%\doi{10.21469/22233792}
%\receivedRus{01.01.2017}
%\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\title{Метрический анализ пространства параметров глубоких нейросетей.}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{Эрнест Р.~Насыров \\
	\texttt{nasyrov.rr@phystech.edu} \\
	%% examples of more authors
    }
	
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\


% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}

%\renewcommand{\baselinestretch}{1.8} 
%КЛЮЧЕАВЫЕ СЛОВА
%\keywords{Временные ряды \and снижение размерности \and релевантность параметров \and пространство параметров \and выбор модели.}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Метрический анализ пространства параметров глубоких нейросетей}
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Эрнест Р.~Насыров},
pdfkeywords={Временные ряды \and снижение размерности \and релевантность параметров \and пространство параметров \and выбор модели.},
}



% Старый Abstract
\begin{comment}
В работе исследуется проблема снижения размерности пространства параметров модели. В ее рамках решается задача восстановления временного ряда. В качестве модели восстановления ряда используются различные автоенкодеры. В работе проводится метрический анализ пространства параметров автоенкодера. Новизна заключается в том, что отдельные параметры модели - случайные величины - собираются в векторы – многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования нашей работы. Этот анализ позволит снизить количество параметров модели, сделать выводы о значимости параметров, произвести их отбор. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов bootstrap и variational inference. Эксперименты проводятся на моделях SSA, RNN и VAE на задачах предсказания синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных.
\end{comment}


% keywords can be removed


\section{Introduction}
\textbf{Ключевые слова:} временные ряды, снижение размерности, релевантность параметров, пространство параметров, выбор модели.
% 1. История + связанные понятия (выбор оптимально модели ...).




%%% С развитием технологий скорость обработки данных растет, они становятся более сложными, большей размерности. 
%%% Привести цифры (из Nature), а пока коммент

%%% ССЫЛКА НА НЕДАВНИЕ РАБОТЫ ПО ТЕМЕ

Высокоразмерные данные избыточны, что представляет сложность для их эффективной обработки и использования. В работе решается задача снижения размерности признакового описания объекта.
%%%, которая привлекла большое внимание ученых.
%%% ССЫЛКИ НА СТАТЬИ/УЧЕНЫХ
Ее базовый принцип состоит в том, чтобы отобразить высокоразмерное признаковое пространство в низкоразмерное, сохраняя важную информацию о данных \citep{jia2022feature}.

На текущий момент известно много методов снижения размерности данных. В работе \citep{ornek2019nonlinear} снижения размерности достигается за счет построения дифференцируемой функции эмбеддинга в низкоразмерное представление, а в \citep{cunningham2014dimensionality} обсуждаюся линейные методы. В работе \citep{isachenko2022quadratic} задача снижения размерности решается для предсказания движения конечностей человека по электрокортикограмме с использованием  метода QPFS (quadratic programming feature selection), учитывающем мультикоррелированность и входных, и целевых признаков.

Наряду с задачей снижения размерности входных данных стоит задача выбора оптимальной структуры модели. В случае оптимизации структуры нейросети, большое внимание уделено изучению признакового пространства модели. В работах \citep{hassibi1993optimal} и \citep{dong2017learning} применяется метод OBS (Optimal Brain Surgeon), состоящий в удалении весов сети с сохранением ее качества аппроксимации, причем выбор удаляемых весов производится с помощью вычисления гессиана функции ошибки по весам.

В статье \citep{grabovoy2019def} приводится метод первого порядка, решающий задачу удаления весов, основанный на нахождении дисперсии градиента функции ошибки по параметру и анализе ковариационной матрицы параметров, а в статье \cite{grabovoy2020intro} нерелевантные веса не удаляют, а прекращают их обучение.


% 2. А вот тут пошла актуальность
Приведенные выше задачи снижения размерности данных и выбора оптимальной структуры нейросети основаны на исследовании пространства входных данных и пространства признаков соответственно. Существенный недостаткок предыдущих работ состоит в том, что в них анализируются \textit{отдельные} параметры (скаляры) моделей и их взаимозависимость. Тем самым не учитывается, что на входные данные действуют \textit{вектора} параметров посредством скалярных произведений, то есть упускается из виду простая \textit{структура} преобразования.

В данной работе решается задача восстановления временного ряда, в рамках которой исследуем проблему снижения размерности пространства параметров модели. Снижение размерности основано на анализе сопряженного пространства ко входному. Оно связывает входное пространство и пространство параметров. 

%%% Статьи Мотренко или Каланчуков в Expert systems with applications (табличка).


Данное исследование в большой степени полагается на простоту устройства глубоких нейросетей, которые являются композицией линейных и простых нелинейных функций (функций активации). Составной блок нейросети описан формой:  \[y=\sigma(\mathsf{W}x + b), \ y, b \in \mathbb{R}^m, \ x \in \mathbb{R}^n, \ \mathsf{W} \in \mathbb{R}^{m \times n}, \ \sigma: \mathbb{R} \to \mathbb{R}.\] Раньше элементы $\mathsf{W}_{ij}$ исследовались по-отдельности, как скаляры. Авторы работы предлагают изучать их как векторы-строки \[\mathbf{w}_1, \dots, \mathbf{w}_m: \mathsf{W} = \begin{pmatrix}
\mathbf{w}_1^{\mathsf{T}}\\
\dots\\
\mathbf{w}_m^{\mathsf{T}}\\
\end{pmatrix}.\] В нейросети эти строки обычно называются \textit{нейронами}. В SSA $\sigma = Id$, а матрица $\mathsf{W}=\mathsf{W}_k$ это приближение истинной матрицы фазовых траекторий $\mathsf{X}$ (матрицы Ганкеля) суммой $k$ элементарных матриц.

% Объяснение фазовой траектории

Обозначим $\mathbf{x} = [x_1, \dots, x_N]^{\mathsf{T}}, x_i \in \mathbb{R}$ ~--- временной ряд, $1 \le n \le N$ ~--- ширина окна. Точка $\mathbf{x}_t = [x_t, \dots, x_{t + n - 1}]^{\mathsf{T}}$ является точкой фазовой траектории временного ряда в траекторном пространстве $\mathbb{H}_{\mathbf{x}} \subset \mathbb{R}^n$. 

Предполагается, что каждая точка фазовой траектории распределена нормально вокруг своего матожидания. Тогда и временной ряд является случайным, поэтому результат обучения модели на нем, то есть параметры обученной модели, будут случайными.

%Тогда обучающая выборка $\mathfrak{D}=(\mathbf{X}, \mathbf{y})$ это набор случайных величин, поэтому и результат обучения модели на ней, то есть веса модели тоже будут случайными. 

В работе исследуется положение случайных векторов параметров модели $\mathbf{w}_i$ в метрическом пространстве. С помощью методов бутстрэпа и вариационного вывода \citep{hastie2009elements} оцениваются их матожидания $\mathbf{e}_i=\mathsf{E}(\mathbf{w}_i)$ и ковариационные матрицы $\mathsf{D}(\mathbf{w}_i) = \mathbf{A}^{-1}_i$. Мы работаем в гипотезе, что эти векторы $\mathbf{w}_i$ распределены нормально, таким образом пара $(\mathbf{e}_i, \mathbf{A}^{-1}_i)$ полностью описывает вероятностное распределение вектора $\mathbf{w}_i$.

\begin{wrapfigure}{r}{0.30\textwidth}

\includegraphics[width=0.25\textwidth]{gaussian_mixture.jpg}
\caption{Смесь гауссианов трех 2-х мерных векторов.}
\label{ris:gauss_mixture}
\end{wrapfigure}

% ~\cite{a, b, c}

В качестве графического анализа пространства изображаются положения этих векторов как смеси гауссианов. На рис. \ref{ris:gauss_mixture} изображены плотности функции распределения трех гауссовских векторов (вертикальная ось) в зависимости от их положения на плоскости. В каждой точке плотность равна сумме плотностей трех распределений, отнормированная таким образом, чтобы площадь под графиком равнялась ~$1$. Точкам максимума куполов распределения соответствуют матожидания векторов, а их форма определяется матрицей ковариации $A$ (добавить стиль). Таким образом, чем ниже и шире 'купол', тем больше дисперсия и наоборот.

\begin{wrapfigure}{l}{0.30\textwidth}
\includegraphics[width=0.25\textwidth]{gaussian_conf_area.jpg}
\caption{Доверительные области 3-х мерных векторов.}
\label{ris:gauss_conf_area}
\end{wrapfigure}


На рис. \ref{ris:gauss_conf_area} изображены эллипсы, соответствующие $95\%$ доверительным областям для гауссовских векторов в 3-х мерном пространстве. Чем больше ширина эллипса вдоль направления, тем больше дисперсия вектора по этому направлению.

% Картинки, иллюстрирующие сказанное выше

Уменьшение размерности достигается за счет метрического анализа пространства векторов-параметров путем отбора релевантных строк (с малой дисперсией), замены мультикоррелирующих строк на их линейную композицию с помощью обобщения алгоритма QPFS, изучения структуры сообществ строк.

В качестве базовых моделей используются SSA (\citep{golyandina2001analysis}), нелинейный PCA, RNN (\citep{bronstein2021geometric}), VAE (\citep{kingma2019introduction}) и Neural ODE (\citep{chen2018neural}).

Задача восстановления временного ряда решается на синтетических данных зашумленного $\sin$, данных показания акселерометра в датасете MotionSense3 \citep{malekzadeh2018protecting}, периодичных видеоданных. 

\section{Problem statement}
Пусть имеется множество из $m$ временных рядов $\mathcal{S}=\{\mathbf{s}_1, \dots, \mathbf{s}_{m}\}, \mathbf{s}_i = [\mathbf{s}_i^1, \dots, \mathbf{s}_i^T], \mathbf{s}_i^j \in \mathbb{R}$, где $n$ ~--- длина сигналов. Каждый временной ряд ~--- последовательность измерений величины в течение времени.

\begin{definition}
Временное представление $\mathbf{x}_t = [\mathbf{s}_1^t, \dots, \mathbf{s}_m^t]^{\mathsf{T}} \in \mathbb{R}^m$ состоит из измерений временных рядов в момент времени $t$.
\end{definition}

\begin{definition}
Предыстория длины $h$ для момента времени $t$ множества временных рядов $\mathcal{S}$ ~--- это матрица $\mathbf{X}_{t,h} = [\mathbf{x}_{t-h+1}, \dots, \mathbf{x}_{t}]^{\mathsf{T}} \in \mathbb{R}^{h \times m}$.
\end{definition}

\begin{definition}
Горизонт прогнозирования длины $p$ для момента времени $t$ множества временных рядов $\mathcal{S}$ ~--- это матрица $\mathbf{Y}_{t, p} = [\mathbf{x}_{t+1}, \dots, \mathbf{x}_{t+p}]^{\mathsf{T}}$.
\end{definition}

\begin{definition}
Прогностическая модель $\mathbf{f}^{\mathsf{AR}}: \mathbb{R}^{h \times m} \to \mathbb{R}^{p \times m}$ является авторегрессионной моделью, которая по предыстории $\mathbf{X}_{t, h}$ предсказывает горизонт планирования $\mathbf{Y}_{t, p}$.
\end{definition}

\begin{comment}
%%% [определения в научных статьях - немного избыточно. Лучше ПОТОМ убрать.]
\end{comment}

Решается задача авторегрессионного декодирования. Она состоит в построении прогностической модели $\mathbf{f}^{\mathsf{AR}}$, дающий \textit{горизонт прогнозирования} множества временных рядов по \textit{предыстории} того же множества рядов. В дальнейшем будем считать, что восстанавливаем 1 временной ряд, то есть что $m=1$.

Обозначим множество всех одномерных временных рядов через $\mathbb{S}$: \[\mathbb{S} = \bigcup\limits_{n=1}^{+\infty}\{[s_1, \dots, s_n] \in \mathbb{R}^{n}\}.\] Тогда прогностическая модель это функция $f^{\mathsf{AR}}: \mathbb{S} \to \mathbb{R}$. Изначальный временной ряд $\mathbf{s} = [s_1, \dots, s_T]$ делится на две части $\mathbf{s} = [\mathbf{s}^H|\mathbf{s}^T], \mathbf{s}^H = [s_1, \dots, s_h], \mathbf{s}^T = [s_{h+1}, \dots, s_T]$. Задача состоит в том, чтобы предсказать $\mathbf{s}^T$ с максимальной точностью. Предсказание происходит следующим образом:

\begin{enumerate}
    \item С помощью модели $f$ предсказывается $\hat{s}_{h+1}$.
    \item Предсказанный элемент $\hat{s}_{h+1}$ вместе с исходным временным рядом $\mathbf{s}^H$ подаются на вход $f$ для предсказания $\hat{s}_{h+2}$.
    \item Шаги $1-2$ повторяются, пока не будет предсказан весь $\hat{\mathbf{s}}^T=[\hat{s}_{h+1}, \dots, \hat{s}_{T}]$.
\end{enumerate}

Для упрощения нотации обозначим $f(\mathbf{s}^H)=\hat{\mathbf{s}}^T = [\hat{s}_{t+1}, \dots, \hat{s}_{T}]$.

Модель $f = f(\mathbf{w}, \mathbf{s}), \mathbf{w} \in \mathbb{W}, \mathbf{s}=[s_1, \dots, s_t]\in \mathbb{R}^t$ выбирается из некоего параметрического семейства.
Параметры модели выбираются таким образом, чтобы минимизировать функцию ошибки $S=S(\mathbf{w}|\mathbf{s},f)$:
$$
\mathbf{w^*} = \argmin\limits_{\mathbf{w} \in \mathbb{W}} S(\mathbf{w}|\mathbf{s},f).
$$


В работе будет использоваться функция ошибки MSE, то есть 
$$
S(\mathbf{w}|\mathbf{s},f) = \sum\limits_{t=h+1}^{T}(\mathbf{s}_t - \hat{\mathbf{s}}_t)^2.
$$.

В качестве входных данных модели получают матрицу фазовых траекторий. Она строится по временному ряду $\mathbf{s} = [s_1, \dots, s_{T}]$ и ширине окна $h$ следующим образом:

$\mathbf{X} = \begin{pmatrix}
s_1 & \dots & s_{k}\\
s_2 & \dots & s_{k+1}\\
\dots & \dots & \dots\\
s_{h} & \dots & s_{T}\\
\end{pmatrix} = \big[X_1 : \dots : X_k\big], k=T-h+1, X_i = [s_i, s_{i+1}, \dots, s_{i + h - 1}] \in \mathbb{R}^h$. 

Траекторная матрица является матрицей Ганкеля, так как каждая диагональ вида $i+j=const$ содержит одинаковые элементы. Векторы $X_i$ являются точками фазовой траектории сигнала. 

В качестве альтернативных моделей для восстановления временного ряда будем использовать 1) SSA, 2) двуслойную нейросеть с ортогональными линейными преобразованиями (нелинейный PCA), 3) RNN и 4) Neural ODE. Остановимся на каждой из них подробнее.

\subsection{SSA}

В модели SSA восстановление временного ряда получается за счет разложения матрицы фазовой траектории в сумму одноранговых матриц. 

Далее вычисляются собственные значения и соответствующие собственным подпространствам ортонормированные системы векторов матрицы \[\mathbf{S}=XX^T \in \mathbb{R}^{h \times h}.\] Обозначим через \[\lambda_1 \ge \dots \ge \lambda_L \ge 0\] собственные значения $\mathbf{S}$, \[u_1, \dots u_h\] ортонормированную систему собственных векторов, соответствующую собственным значениям, \[v_i = \frac{\mathbf{X}^Tu_i}{\sqrt{\lambda_i}} \ (i=1, \dots, h).\] Тогда \[\mathbf{X} = \mathbf{X}_1 + \dots + \mathbf{X}_d, \mathbf{X}_i = \sqrt{\lambda_i}u_iv_i^T\] ~--- SVD-разложение $\mathbf{X}$. Далее происходит группировка матриц $\mathbf{X}_i$, их ганкелизация и восстанавливается матрица сигнала \[\hat{\mathbf{X}} = \mathbf{\tilde{X}}_{I_1} + \dots + \mathbf{\tilde{X}}_{I_m}, \ I_1 \sqcup \dots \sqcup I_m \subset \{1, \dots, h\}, \ \mathbf{\tilde{X}}_{I} = hank\big(\sum\limits_{i \in I}\mathbf{X}_i\big).\] Где $\text{hank}(X)$ ~--- ганкелизация матрицы $X$, состоящая в том, что на каждой диагонали вида $i+j=const$ все элементы заменяются на их среднее арифметическое. Параметрами модели SSA являются множества $I_1, \dots, I_m$.

\subsection{Нейросеть}
Модель двуслойной нейросети с ортогональными преобразованиями задается следующим образом: \[\mathbf{f}(\mathbf{x}) = \sigma(\mathbf{W}_1^{\mathsf{T}}\cdot \sigma(\mathbf{W}_2^{\mathsf{T}}\mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2),\] где  \[\mathbf{x} \in \mathbb{R}^{h}, \ \mathbf{W}_2 \in \mathbb{R}^{h \times d}, \mathbf{W}_1 \in \mathbb{R}^{d \times h} \ b_1 \in \mathbb{R}^h, \ b_2 \in \mathbb{R}^{h}; \mathbf{W}_1^{\mathsf{T}}\mathbf{W}_1=\mathsf{I}, \mathbf{W}_2\mathbf{W}_2^{\mathsf{T}}=\mathsf{I}.\] Последние два условия гарантируют, что преобразования будет ортогональным. Здесь нейросеть восстанавливает значения сигнала длиной в $h$ моментов времени.

\subsection{RNN}
Модель RNN задается следующим образом: \[ h_t = \sigma(W \cdot h_{t-1} + V \cdot \mathbf{x}_t),\]
\[ s_{t+1} = tanh(w_o^{\mathsf{T}} \cdot h_t), \] где $h_{t} \in \mathbb{R}^{d}$ ~--- скрытое состояние RNN в момент времени $t$, $W \in \mathbb{R}^{d \times d}, V \in \mathbb{R}^{d \times h}. \\
\mathbf{x}_t = [s_{t - h + 1}, \dots, s_{t}] \in \mathbb{R}^h$ - временной ряд, подающийся на вход модели, $w_o \in \mathbb{R}^{d}, s_{t+1} \in \mathbb{R}$ ~--- прогноз значения сигнала в момент времени $t+1$. Параметрами модели являются матрицы $W, V, w_0$ а также начальное скрытое состояние $h_0$, которое мы зафиксируем $h_0=0$.

О входном временном ряде выдвинута гипотеза о том, что точки на фазовой траектории распределены по нормальному закону, то есть что $\mathbf{x}_t = [s_t, \dots, s_{t + h - 1}] \sim \mathcal{N}(\hat{\mathbf{x}}_t, \mathbf{B})$, где $\hat{\mathbf{x}}_t$ ~--- это матожидание точки фазовой траектории в момент времени $t$, а $\mathbf{B}$ ~--- матрица ковариации. Мы также предполагаем, что [ссылаться на формулу, где впервые введена W]$\mathbf{w} \sim \mathcal{N}(\hat{\mathbf{w}}, \mathbf{A})$. 

В работе оцениваются матожидание вектора параметров модели $\hat{\mathbf{w}}$, а также матрица ковариации параметров $\mathbf{A}$ с помощью методов бутстрепа и вариационного вывода.

Для метрического анализа пространства параметров выбираются набор множеств индексов $\mathcal{I}_1 \sqcup \dots \sqcup \mathcal{I}_k \subset \{1, \dots, len(\mathbf{w})\}$, рассматриваются соответствующие им подвектора параметров $\mathbf{w}_{\mathcal{I}_1}, \dots, \mathbf{w}_{\mathcal{I}_k}$
Считается, что каждый $\mathbf{w}_{\mathcal{I}_j}$ соответствует <<смысловой единице>> модели. В нейросетевых моделях это строки матрицы линейного преобразования пространства параметров $W$, которые обычно называют \textit{нейронами}.

Для каждого $\mathbf{w}_{\mathcal{I}}$ оценивается матожидание $\hat{\mathbf{w}}_{\mathcal{I}}$ и ковариационная матрица $\mathbf{A}_{\mathcal{I}}$, с помощью которых и проводится метрический анализ пространства параметров.

%%%[Написать что-нибудь про Neural ODE].

\section{Computational experiment}
\subsection{SSA}
Скалярный временной ряд $\mathbf{\tilde{x}} = \mathbf{x} + \varepsilon$ состоит из $n=500$ значений зашумленного синуса, измеренного в точках интервала $[-\pi, \pi]$ с равномерным шагом. Использован шум из распределения $\varepsilon \sim \mathcal{N}(0, 0.2)$. Изображение данных представлено на рис. \ref{ris:noiced_sin}.

\begin{wrapfigure}{l!}{0.42\textwidth}
\includegraphics[width=0.4\textwidth]{noiced_sin.jpg}
\caption{Зашумленный синус.}
\label{ris:noiced_sin}
\end{wrapfigure}

С помощью метода SSA построена зависимость точности восстановления временного ряда $\mathbf{x}$ из шумного ряда $\mathbf{\tilde{x}}$ в зависимости от ширины окна $h$ и количества главных компонент $k$ при восстановлении матрицы $\mathbf{X}$.

В качестве критерия качества использовались MSE и MAPE, замеренные между восстановленным рядом $\mathbf{\hat{x}}$ и истинным значением ряда $\mathbf{x}$. Результаты представлены на рис. \ref{fig:ssa_mse} и рис. \ref{fig:ssa_mape}.

\captionsetup[subfigure]{labelsep=colon}

\begin{figure}[ht]%
    \centering
    \subfloat[MSE для различных значений значения $h$ (emb\_size) и $k$. Черной линией выделен MSE между чистым и зашумленным рядом.]{\includegraphics[width=7.5cm]{jmlda-guides/SSA_MSE.jpg}}%
    %\caption{MSE для различных значений значения $h$ (emb\_size) и $k$. Черной линией выделен MSE между чистым и зашумленным рядом.}%
    \label{fig:ssa_mse}%
    \qquad
    \subfloat[MAPE для различных значений значения $h$ (emb\_size) и $k$. Черной линией выделен MAPE между чистым и зашумленным рядом.]{\includegraphics[width=7.5cm]{jmlda-guides/SSA_MAPE.jpg} }%
    %\caption{MAPE для различных значений значения $h$ (emb\_size) и $k$. Черной линией выделен MAPE между чистым и зашумленным рядом.}%
    \label{fig:ssa_mape}%
\end{figure}

Различная длина графиков обусловлена тем, что длина окна $h$ не может быть больше чем количество слагаемых в SVD-восстановлении ряда. Видно, что истинное значение ряда с большой точностью восстанавливается при достатоно небольшом количестве слагаемых ($<20$), что соответствует точке минимума всех графиков на левом рисунке. Чем больше компонет используется, тем больше становится ошибка и при количестве компонент равном длине окна MSE становится таким же, как и MSE между истинным значением ряда и его шумной версией (графики приближаются к горизонтальной пунктирной линии).


Полученные результаты подтверждают практическую состоятельность SSA: алгоритм восстанавливает главный тренд при небольшом количестве сингулярных слагаемых $\mathbf{X}_i$.


\subsection{Двуслойная нейросеть с ортогональными преобразованиями}
Скалярный временной ряд $\mathbf{\tilde{x}} = \mathbf{x} + \varepsilon$ состоит из $n=500$ значений зашумленного синуса, измеренного в точках интервала $[-4\pi, 4\pi]$ с равномерным шагом. Использован шум из распределения $\varepsilon \sim \mathcal{N}(0, 0.2)$. Изображение данных представлено на рис. \ref{ris:noiced_sin_2nn}.

\begin{wrapfigure}{l!}{0.42\textwidth}
\includegraphics[width=0.4\textwidth]{noiced_sin_2nn.jpg}
\caption{Зашумленный синус.}
\label{ris:noiced_sin_2nn}
\end{wrapfigure}

С помощью нейросети решалась заадча восстановления временного ряда. Скрытая размерность нейросети $d=3$, размерность траекторного пространства $h=20$.

На случайных подвыборках обучающей выборки были обучены $N=100$ нейросетей на 25 эпохах. Оценены матожидания и матрицы ковариаций столбцов параметра $\mathbf{W}_1 \in \mathbf{R}^{3 \times 20}$. Полученные матрицы визуализированы в 3-х мерном пространстве рис~\ref{ris:2nn_total_gauss_3d_conf}.

\newpage

\begin{wrapfigure}{!ht}{0.4\textwidth}
\includegraphics[width=0.4\textwidth]{jmlda-guides/2nn_total_gauss_3d_conf.jpg}
\caption{Доверительные области 3-х мерных векторов двуслойной нейросети арихитектуры 20-3-20.}
\label{ris:2nn_total_gauss_3d_conf}
\end{wrapfigure}

Все параметры-нейроны на рис.~\cite{ris:2nn_total_gauss_3d_conf} имеют форму вытянутых эллипсов, расположенных на удалении друг от друга. Часть эллипсов вытянута в одну и ту же сторону (уменьшения оси $OY$). В целом все эти нейроны кажутся довольно независимыми и не имеющими выраженной структуры сообществ.

В следующем эксперимента с нейросетью бралась архитектура $10-3-20$, которая означает, что на вход подаются $h_1=10$-мерные векторы, размерность скрытого пространства $d=3$, а восстановить нужно $h_1=20$ моментов времени.

На случайных подвыборках обучающей выборки были обучены $N=100$ нейросетей на 70 эпохах. Оценены матожидания и матрицы ковариаций столбцов параметра $\mathbf{W}_1 \in \mathbf{R}^{3 \times 10}$. Полученные матрицы визуализированы в 3-х мерном пространстве рис~\ref{ris:3d_var_10_3_20}.

\begin{wrapfigure}{!ht}{0.4\textwidth}
\includegraphics[width=0.4\textwidth]{jmlda-guides/3d_var_10_3_20.jpg}
\caption{Доверительные области 3-х мерных векторов двуслойной нейросети арихитектуры 10-3-20.}
\label{ris:3d_var_10_3_20}
\end{wrapfigure}

Здесь, в отличие от предыдущего эксперимента наблюдается разделение на 3 сообщества, соответствующие оранжевому, красному и фиолетовому эллипсоиду, которые имеют самую большую дисперсию среди нейронов (так как они большего объема).

Это частично подтверждает гипотезу о существовании структуры сообществ у нейронов.
\newpage

\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .





\end{document}
