\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage[english,russian]{babel}

\title{Метрический анализ пространства параметров глубоких нейросетей.}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{Эрнест Р.~Насыров \\
	\texttt{nasyrov.rr@phystech.edu} \\
	%% examples of more authors
    }
	
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\


% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}
\renewcommand{\baselinestretch}{1.8} 

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Метрический анализ пространства параметров глубоких нейросетей}
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Эрнест Р.~Насыров},
pdfkeywords={нет слова},
}

\begin{document}
\maketitle

\begin{abstract}

Исследуется проблема снижения размерности пространства параметров модели машинного обучения. Решается задача восстановления временного ряда. Для восстановления используются авторегресионные модели: линейные, автоенкодеры, реккурентные сети - с непрерывным и дискретным временем. Проводится метрический анализ пространства параметров модели.  Предполагается, что отдельные параметры модели, случайные величины, собираются в векторы, многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования нашей работы.  Этот анализ снижает число параметров модели, оценивает значимости параметров, отбирая их. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов \textit{бутстрэпа} и \textit{вариационного} вывода. Эксперименты проводятся на задачах восстановления синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных. Для восстановления применяются модели SSA, RNN, VAE, Neural ODE.


% Старый Abstract
\begin{comment}
В работе исследуется проблема снижения размерности пространства параметров модели. В ее рамках решается задача восстановления временного ряда. В качестве модели восстановления ряда используются различные автоенкодеры. В работе проводится метрический анализ пространства параметров автоенкодера. Новизна заключается в том, что отдельные параметры модели - случайные величины - собираются в векторы – многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования нашей работы. Этот анализ позволит снизить количество параметров модели, сделать выводы о значимости параметров, произвести их отбор. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов bootstrap и variational inference. Эксперименты проводятся на моделях SSA, RNN и VAE на задачах предсказания синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных.
\end{comment}
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}

% 1. История + связанные понятия (выбор оптимально модели ...).




%%% С развитием технологий скорость обработки данных растет, они становятся более сложными, большей размерности. 
%%% Привести цифры (из Nature), а пока коммент

%%% ССЫЛКА НА НЕДАВНИЕ РАБОТЫ ПО ТЕМЕ

Высокоразмерные данные избыточны, что представляет сложность для их эффективной обработки и использования. В работе решается задача снижения размерности признакового описания объекта.
%%%, которая привлекла большое внимание ученых.
%%% ССЫЛКИ НА СТАТЬИ/УЧЕНЫХ
Ее базовый принцип состоит в том, чтобы отобразить высокоразмерное признаковое пространство в низкоразмерное, сохраняя важную информацию о данных \citep{jia2022feature}.

На текущий момент известно много методов снижения размерности данных. В работе \citep{ornek2019nonlinear} снижения размерности достигается за счет построения дифференцируемой функции эмбеддинга в низкоразмерное представление, а в \citep{cunningham2014dimensionality} обсуждаюся линейные методы. В работе \citep{isachenko2022quadratic} задача снижения размерности решается для предсказания движения конечностей человека по электрокортикограмме с использованием  метода QPFS, учитывающем мультикоррелированность и входных, и целевых признаков.

Наряду с задачей снижения размерности входных данных стоит задача выбора оптимальной структуры модели. В случае оптимизации структуры нейросети, большое внимание уделено изучению признакового пространства модели. В работах \citep{hassibi1993optimal} и \citep{dong2017learning} применяется метод OBS (Optimal Brain Surgeon), состоящий в удалении весов сети с сохранением ее качества аппроксимации, причем выбор удаляемых весов производится с помощью вычисления гессиана функции ошибки по весам.

В статье \citep{грабовой2019определение} приводится метод первого порядка, решающий задачу удаления весов, основанный на нахождении дисперсии градиента функции ошибки по параметру и анализе ковариационной матрицы параметров, а в статье \citep{грабовой2020введение} нерелевантные веса не удаляют, а прекращают их обучение.


% 2. А вот тут пошла актуальность
Приведенные выше задачи понижения размерности данных и выбора оптимальной структуры нейросети основаны на исследовании пространства входных данных и пространства признаков соответственно. На взгляд авторов статьи, существенный недостаткок предыдущих работ состоит в том, что в них анализируются \textit{отдельные} параметры (скаляры) моделей и их взаимозависимость. Тем самым не учитывается, что на входные данные действуют \textit{вектора} параметров посредством скалярных произведений, то есть упускается из виду простая \textit{структура} преобразования.

В данной работе мы решаем задачу восстановления временного ряда, в рамках которой занимаемся проблемой снижения пространства параметров модели, основанном на анализе сопряженного пространства ко входному, которое связывает входное пространство и пространство параметров. 

Наше исследование в большой степени полагается на простоту устройства глубоких нейросетей, которые являются композицией линейных и простых нелинейных функций (функций активации). Составной блок нейросети описан формой:  $y=\sigma(\mathsf{W}x), y \in \mathbb{R}^m, x \in \mathbb{R}^n, \mathsf{W} \in \mathbb{R}^{m \times n}, \sigma: \mathbb{R} \to \mathbb{R}$. Раньше элементы $\mathsf{W}_{ij}$ исследовались по-отдельности, как скаляры. Авторы работы предлагают изучать их как векторы-строки $\mathbf{w}_1, \dots, \mathbf{w}_m: \mathsf{W} = \begin{pmatrix}
\mathbf{w}_1^{\mathsf{T}}\\
\dots\\
\mathbf{w}_m^{\mathsf{T}}\\
\end{pmatrix}$. В нейросети эти строки обычно называются \textit{нейронами}. В SSA $\sigma = Id$, а матрица $\mathsf{W}=\mathsf{W}_k$ это приближение истинной матрицы фазовых траекторий $\mathsf{X}$ (матрицы Ганкеля) суммой $k$ элементарных матриц.

% Объяснение фазовой траектории

Пусть $\mathbf{x} = [x_1, \dots, x_N]^{\mathsf{T}}, x_i \in \mathbb{R}$ - временной ряд, $1 \le n \le N$ - ширина окна. Точка $\mathbf{x}_t = [x_t, \dots, x_{t + n - 1}]^{\mathsf{T}}$ является точкой фазовой траектории временного ряда в траекторном пространстве $\mathbb{H}_{\mathbf{x}} \subset \mathbb{R}^n$. 

Предполагается, что каждая точка фазовой траектории распределена нормально вокруг своего матожидания. Тогда и временной ряд является случайным, поэтому результат обучения модели на нем, то есть параметры обученной модели, будут случайными.

%Тогда обучающая выборка $\mathfrak{D}=(\mathbf{X}, \mathbf{y})$ это набор случайных величин, поэтому и результат обучения модели на ней, то есть веса модели тоже будут случайными. 

В работе исследуется положение случайных векторов параметров модели $\mathbf{w}_i$ в метрическом пространстве. С помощью методов бутстрэпа и вариационного вывода \citep{hastie2009elements} оцениваются их матожидания $\mathbf{e}_i=E(\mathbf{w}_i)$ и ковариационные матрицы $D(\mathbf{w}_i) = \mathbf{A}^{-1}_i$. Мы работаем в гипотезе, что эти векторы $\mathbf{w}_i$ распределены нормально, таким образом пара $(\mathbf{e}_i, \mathbf{A}^{-1}_i)$ полностью описывает вероятностное распределение вектора $\mathbf{w}_i$. 

\begin{wrapfigure}{r}{0.30\textwidth}

\includegraphics[width=0.25\textwidth]{gaussian_mixture.jpg}
\caption{Смесь гауссианов трех 2-х мерных векторов.}
\label{ris:gauss_mixture}

\end{wrapfigure}
В качестве графического анализа пространства изображаются положения этих векторов как смеси гауссианов \ref{ris:gauss_mixture}.  На рисунке изображены плотности функции распределения трех гауссовских векторов (вертикальная ось) в зависимости от их положения на плоскости. В каждой точке плотность равна сумме плотностей трех распределений, отнормированная таким образом, чтобы площадь под графиком равнялась $1$. Центрам 'куполов' соответствуют матожидания векторов, а их форма определяется матрицей ковариации. Таким образом, чем ниже и шире 'купол', тем больше дисперсия и наоборот.

\begin{wrapfigure}{l}{0.30\textwidth}
\includegraphics[width=0.25\textwidth]{gaussian_conf_area.jpg}
\caption{Доверительные области 3-х мерных векторов.}
\label{ris:gauss_conf_area}
\end{wrapfigure}
Также изображается $95\%$ доверительная область каждого вектора \ref{ris:gauss_conf_area}. На картинке изображены эллипсы, соответствующие доверительным областям для гауссовских векторов в 3-х мерном пространстве. Чем больше ширина эллипса вдоль направления, тем больше дисперсия вектора по этому направлению.

% Картинки, иллюстрирующие сказанное выше

Уменьшение размерности достигается за счет метрического анализа пространства векторов-параметров путем отбора релевантных строк (с малой дисперсией), замены мультикоррелирующих строк на их линейную композицию с помощью обобщения алгоритма QPFS, изучения структуры сообществ строк.

В качестве базовых моделей используются SSA (\citep{golyandina2001analysis}), нелинейный PCA, RNN (\citep{bronstein2021geometric}), VAE (\citep{kingma2019introduction}) и Neural ODE (\citep{chen2018neural}).

Задача восстановления временного ряда решается на синтетических данных зашумленного $sin$, данных показания акселерометра в датасете MotionSense3 \citep{malekzadeh2018protecting}, периодичных видеоданных. 


% А дальше немного про SSA, RNN и VAE (И посмотреть на модели Стрижова!!!)
\newpage

\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .





\end{document}
