\documentclass[12pt, twoside]{article}
\usepackage{jmlda}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[english,russian]{babel}

\newcommand{\hdir}{.}
\newtheorem{definition}{Definition}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\baselinestretch}{1.8} 

\begin{document}

% Здесь можно определять собственные команды, они будут действовать только внутри статьи:
\newenvironment{coderes}%
    {\medskip\tabcolsep=0pt\begin{tabular}{>{\small}l@{\quad}|@{\quad}l}}%
    {\end{tabular}\medskip}


\title{Метрический анализ пространства параметров глубоких нейросетей}
\author{Эрнест Р.~Насыров,  Вадим В.~Стрижов}
\email{nasyrov.rr@phystech.edu}
%organization{ФИЦ <<Информатика и управление>> РАН, г.~Москва, ул.~Вавилова, 44/2}
\abstract{
Исследуется проблема снижения размерности пространства параметров модели машинного обучения. Решается задача восстановления временного ряда. Для восстановления используются авторегресионные модели: линейные, автоенкодеры, реккурентные сети ~--- с непрерывным и дискретным временем. Проводится метрический анализ пространства параметров модели.  Предполагается, что отдельные параметры модели, случайные величины, собираются в векторы, многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования данной работы.  Этот анализ снижает число параметров модели, оценивает значимости параметров, отбирая их. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов \textit{бутстрэпа} и \textit{вариационного вывода}. Эксперименты проводятся на задачах восстановления синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных. Для восстановления применяются модели SSA (singular spectrum analysis), нелинейного PCA (principal component analysis), RNN (reccurent neural network), Neural ODE (neural ordinary differential equations).

\bigskip
\noindent
\textbf{Ключевые слова}: \emph {Временные ряды; снижение размерности; релевантность параметров; пространство параметров; выбор модели.}
}

\titleEng{Style guide for authors}
\authorEng{JMLDA editorial board}
\organizationEng{Federal Research Center ``Computer Science and Control'' of RAS, 44/2~Vavilova~st., Moscow, Russia}
\abstractEng{
    This document explains how to prepare papers using \LaTeXe\ typesetting system and \texttt{jmlda.sty} package.
}
%\doi{10.21469/22233792}
%\receivedRus{01.01.2017}
%\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\title{Метрический анализ пространства параметров глубоких нейросетей.}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{Эрнест Р.~Насыров 
 Вадим В.~Стрижов \\
	\texttt{nasyrov.rr@phystech.edu} \\
	%% examples of more authors
 \AND
 Вадим В.~Стрижов \\
    }
    \AND
\author{Вадим В.~Стрижов \\
	%% examples of more authors
    }
	
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\


% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}

%\renewcommand{\baselinestretch}{1.8} 
%КЛЮЧЕАВЫЕ СЛОВА
%\keywords{Временные ряды \and снижение размерности \and релевантность параметров \and пространство параметров \and выбор модели.}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Метрический анализ пространства параметров глубоких нейросетей}
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Эрнест Р.~Насыров},
pdfkeywords={Временные ряды \and снижение размерности \and релевантность параметров \and пространство параметров \and выбор модели.},
}



% Старый Abstract
\begin{comment}
В работе исследуется проблема снижения размерности пространства параметров модели. В ее рамках решается задача восстановления временного ряда. В качестве модели восстановления ряда используются различные автоенкодеры. В работе проводится метрический анализ пространства параметров автоенкодера. Новизна заключается в том, что отдельные параметры модели - случайные величины - собираются в векторы – многомерные случайные величины, анализ взаимного расположения которых в пространстве и представляет предмет исследования нашей работы. Этот анализ позволит снизить количество параметров модели, сделать выводы о значимости параметров, произвести их отбор. Для определения положения вектора параметров в пространстве оцениваются его матожидание и матрица ковариации с помощью методов bootstrap и variational inference. Эксперименты проводятся на моделях SSA, RNN и VAE на задачах предсказания синтетических временных рядов, квазипериодических показаний акселерометра, периодических видеоданных.
\end{comment}


% keywords can be removed


\section{Introduction}
\textbf{Ключевые слова:} временные ряды, снижение размерности, релевантность параметров, пространство параметров, выбор модели.
% 1. История + связанные понятия (выбор оптимально модели ...).




%%% С развитием технологий скорость обработки данных растет, они становятся более сложными, большей размерности. 
%%% Привести цифры (из Nature), а пока коммент

%%% ССЫЛКА НА НЕДАВНИЕ РАБОТЫ ПО ТЕМЕ

Высокоразмерные данные избыточны, что представляет сложность для их эффективной обработки и использования. В работе решается задача снижения размерности признакового описания объекта.
%%%, которая привлекла большое внимание ученых.
%%% ССЫЛКИ НА СТАТЬИ/УЧЕНЫХ
Ее базовый принцип состоит в том, чтобы отобразить высокоразмерное признаковое пространство в низкоразмерное, сохраняя важную информацию о данных \citep{jia2022feature}.

На текущий момент известно много методов снижения размерности данных. В работе \citep{ornek2019nonlinear} снижения размерности достигается за счет построения дифференцируемой функции эмбеддинга в низкоразмерное представление, а в \citep{cunningham2014dimensionality} обсуждаюся линейные методы. В работе \citep{isachenko2022quadratic} задача снижения размерности решается для предсказания движения конечностей человека по электрокортикограмме с использованием  метода QPFS (quadratic programming feature selection), учитывающем мультикоррелированность и входных, и целевых признаков.

Наряду с задачей снижения размерности входных данных стоит задача выбора оптимальной структуры модели. В случае оптимизации структуры нейросети, большое внимание уделено изучению признакового пространства модели. В работах \citep{hassibi1993optimal} и \citep{dong2017learning} применяется метод OBS (Optimal Brain Surgeon), состоящий в удалении весов сети с сохранением ее качества аппроксимации, причем выбор удаляемых весов производится с помощью вычисления гессиана функции ошибки по весам.

В статье \citep{grabovoy2019def} приводится метод первого порядка, решающий задачу удаления весов, основанный на нахождении дисперсии градиента функции ошибки по параметру и анализе ковариационной матрицы параметров, а в статье \citep{grabovoy2020intro} нерелевантные веса не удаляют, а прекращают их обучение.


% 2. А вот тут пошла актуальность
Приведенные выше задачи снижения размерности данных и выбора оптимальной структуры нейросети основаны на исследовании пространства входных данных и пространства признаков соответственно. Существенный недостаткок предыдущих работ состоит в том, что в них анализируются \textit{отдельные} параметры (скаляры) моделей и их взаимозависимость. Тем самым не учитывается, что на входные данные действуют \textit{векторы} параметров посредством скалярных произведений, то есть упускается из виду простая \textit{структура} преобразования.

В данной работе решается задача восстановления временного ряда, в рамках которой исследуем проблему снижения размерности пространства параметров модели. Снижение размерности основано на анализе сопряженного пространства ко входному. Оно связывает входное пространство и пространство параметров. 

%%% Статьи Мотренко или Каланчуков в Expert systems with applications (табличка).

В данном исследовании мы будем рассматривать нейросети простейшей структуры - которые являются композицией линейных и простых нелинейных функций (функций активации). Их составной блок описан формой:  \[\mathbf{y}=\sigma(\mathbf{W}\mathbf{x} + \mathbf{b}), \ \mathbf{y}, \mathbf{b} \in \mathbb{R}^m, \ \mathbf{x} \in \mathbb{R}^n, \ \mathbf{W} \in \mathbb{R}^{m \times n}, \ \sigma: \mathbb{R} \to \mathbb{R}.\] 

В методах OBS, OBD и в статье \citep{grabovoy2019def} элементы $\mathbf{W}_{ij}$ исследовались по-отдельности, как скаляры. Авторы работы предлагают изучать их как векторы-строки \[\mathbf{w}_1, \dots, \mathbf{w}_m: \mathbf{W} = \begin{pmatrix}
\mathbf{w}_1^{\mathsf{T}}\\
\dots\\
\mathbf{w}_m^{\mathsf{T}}\\
\end{pmatrix}.\] В нейросети эти строки обычно называются \textit{нейронами}. В SSA (singular spectrum analysis) $\sigma = Id$, а матрица $\mathbf{W}=\mathbf{W}_k$ это приближение истинной матрицы фазовых траекторий $\mathsf{\mathbf{X}}$ (матрицы Ганкеля) суммой $k$ элементарных матриц.

% Объяснение фазовой траектории

Обозначим $\mathbf{x} = [x_1, \dots, x_N]^{\mathsf{T}}, x_i \in \mathbb{R}$ ~--- временной ряд, $1 \le n \le N$ ~--- ширина окна. Точка $\mathbf{x}_t = [x_t, \dots, x_{t + n - 1}]^{\mathsf{T}}$ является точкой фазовой траектории временного ряда в траекторном пространстве $\mathbb{H}_{\mathbf{x}} \subset \mathbb{R}^n$. 

Предполагается, что каждая точка фазовой траектории распределена нормально вокруг своего матожидания. Тогда и временной ряд является случайным, поэтому результат обучения модели на нем, то есть параметры обученной модели, будут случайными.

%Тогда обучающая выборка $\mathfrak{D}=(\mathbf{X}, \mathbf{y})$ это набор случайных величин, поэтому и результат обучения модели на ней, то есть веса модели тоже будут случайными. 

В работе исследуется положение случайных векторов параметров модели $\mathbf{w}_i$ в метрическом пространстве. С помощью методов бутстрэпа и вариационного вывода \citep{hastie2009elements} оцениваются их матожидания $\mathbf{e}_i=\mathsf{E}(\mathbf{w}_i)$ и ковариационные матрицы $\mathsf{D}(\mathbf{w}_i) = \mathbf{A}^{-1}_i$. Мы работаем в гипотезе, что эти векторы $\mathbf{w}_i$ распределены нормально, таким образом пара $(\mathbf{e}_i, \mathbf{A}^{-1}_i)$ полностью описывает вероятностное распределение вектора $\mathbf{w}_i$.

\begin{wrapfigure}{r}{0.30\textwidth}

\includegraphics[width=0.25\textwidth]{gaussian_mixture.jpg}
\caption{Смесь гауссианов трех 2-х мерных векторов.}
\label{ris:gauss_mixture}
\end{wrapfigure}

% ~\cite{a, b, c}

В качестве графического анализа пространства изображаются положения этих векторов как смеси гауссианов. На рис. \ref{ris:gauss_mixture} изображены плотности функции распределения трех гауссовских векторов (вертикальная ось) в зависимости от их положения на плоскости. В каждой точке плотность равна сумме плотностей трех распределений, отнормированная таким образом, чтобы площадь под графиком равнялась ~$1$. Точкам максимума куполов распределения соответствуют матожидания векторов, а их форма определяется матрицей ковариации $\mathbf{A}^{-1}$. Таким образом, чем ниже и шире <<купол>>, тем больше дисперсия и наоборот.

\begin{wrapfigure}{l}{0.30\textwidth}
\includegraphics[width=0.25\textwidth]{gaussian_conf_area.jpg}
\caption{Доверительные области 3-х мерных векторов.}
\label{ris:gauss_conf_area}
\end{wrapfigure}


На рис. \ref{ris:gauss_conf_area} изображены эллипсы, соответствующие $95\%$ доверительным областям для гауссовских векторов в 3-х мерном пространстве. Чем больше ширина эллипса вдоль направления, тем больше дисперсия вектора по этому направлению.

% Картинки, иллюстрирующие сказанное выше

Уменьшение размерности достигается за счет метрического анализа пространства векторов-параметров путем отбора релевантных строк (с малой дисперсией), замены мультикоррелирующих строк на их линейную композицию с помощью обобщения алгоритма QPFS, изучения структуры сообществ строк.

В качестве базовых моделей используются SSA (singular spectrum analysis)(\citep{golyandina2001analysis}), нелинейный PCA (principal component analysis), RNN (reccurent neural network) (\citep{bronstein2021geometric}), VAE (variational autoencoder)(\citep{kingma2019introduction}) и Neural ODE (neural ordinary differential equations)(\citep{chen2018neural}).

Задача восстановления временного ряда решается на синтетических данных зашумленного $\sin$, данных показания акселерометра в выборке MotionSense3 \citep{malekzadeh2018protecting}, периодичных видеоданных. 

\section{Problem statement}
Пусть имеется множество из $m$ временных рядов $\mathcal{S}=\{\mathbf{s}_1, \dots, \mathbf{s}_{m}\}, \mathbf{s}_i = [\mathbf{s}_i^1, \dots, \mathbf{s}_i^T], \mathbf{s}_i^j \in \mathbb{R}$, где $n$ ~--- длина сигналов. Каждый временной ряд ~--- последовательность измерений величины в течение времени.

\begin{definition}
Временное представление $\mathbf{x}_t = [\mathbf{s}_1^t, \dots, \mathbf{s}_m^t]^{\mathsf{T}} \in \mathbb{R}^m$ состоит из измерений временных рядов в момент времени $t$.
\end{definition}

\begin{definition}
Предыстория длины $h$ для момента времени $t$ множества временных рядов $\mathcal{S}$ ~--- это матрица $\mathbf{X}_{t,h} = [\mathbf{x}_{t-h+1}, \dots, \mathbf{x}_{t}]^{\mathsf{T}} \in \mathbb{R}^{h \times m}$.
\end{definition}

\begin{definition}
Горизонт прогнозирования длины $p$ для момента времени $t$ множества временных рядов $\mathcal{S}$ ~--- это матрица $\mathbf{Y}_{t, p} = [\mathbf{x}_{t+1}, \dots, \mathbf{x}_{t+p}]^{\mathsf{T}}$.
\end{definition}

\begin{definition}
Прогностическая модель $\mathbf{f}^{\mathsf{AR}}: \mathbb{R}^{h \times m} \to \mathbb{R}^{p \times m}$ является авторегрессионной моделью, которая по предыстории $\mathbf{X}_{t, h}$ предсказывает горизонт планирования $\mathbf{Y}_{t, p}$.
\end{definition}

\begin{comment}
%%% [определения в научных статьях - немного избыточно. Лучше ПОТОМ убрать.]
\end{comment}

Решается задача авторегрессионного декодирования. Она состоит в построении прогностической модели $\mathbf{f}^{\mathsf{AR}}$, дающий \textit{горизонт прогнозирования} множества временных рядов по \textit{предыстории} того же множества рядов. В дальнейшем будем считать, что восстанавливаем 1 временной ряд, то есть что $m=1$.

Обозначим множество всех одномерных временных рядов через $\mathbb{S}$: \[\mathbb{S} = \bigcup\limits_{n=1}^{+\infty}\{[s_1, \dots, s_n] \in \mathbb{R}^{n}\}.\] Длину горизонта планирования зафиксируем равной $1$. Тогда прогностическая модель --- это функция $f^{\mathsf{AR}}: \mathbb{S} \to \mathbb{R}$. Изначальный временной ряд $\mathbf{s} = [s_1, \dots, s_T]$ делится на две части $\mathbf{s} = [\mathbf{s}^H|\mathbf{s}^T], \mathbf{s}^H = [s_1, \dots, s_h], \mathbf{s}^T = [s_{h+1}, \dots, s_T]$. Задача состоит в том, чтобы предсказать $\mathbf{s}^T$ с максимальной точностью. Предсказание происходит следующим образом:

\begin{enumerate}
    \item С помощью модели $f$ предсказывается $\hat{s}_{h+1}$.
    \item Предсказанный элемент $\hat{s}_{h+1}$ вместе с исходным временным рядом $\mathbf{s}^H$ подаются на вход $f$ для предсказания $\hat{s}_{h+2}$.
    \item Шаги $1-2$ повторяются, пока не будет предсказан весь $\hat{\mathbf{s}}^T=[\hat{s}_{h+1}, \dots, \hat{s}_{T}]$.
\end{enumerate}

Для упрощения нотации обозначим $f(\mathbf{s}^H)=\hat{\mathbf{s}}^T = [\hat{s}_{t+1}, \dots, \hat{s}_{T}]$.

Модель $f = f(\mathbf{w}, \mathbf{s}), \mathbf{w} \in \mathbb{W}, \mathbf{s}=[s_1, \dots, s_t]\in \mathbb{R}^t$ выбирается из некоего параметрического семейства.
Параметры модели выбираются таким образом, чтобы минимизировать функцию ошибки $S=S(\mathbf{w}|\mathbf{s},f)$:
$$
\mathbf{w^*} = \argmin\limits_{\mathbf{w} \in \mathbb{W}} S(\mathbf{w}|\mathbf{s},f).
$$


В работе будет использоваться функция ошибки MSE, то есть 
$$
S(\mathbf{w}|\mathbf{s},f) = \sum\limits_{t=h+1}^{T}(\mathbf{s}_t - \hat{\mathbf{s}}_t)^2.
$$.

В качестве входных данных модели получают матрицу фазовых траекторий. Она строится по временному ряду $\mathbf{s} = [s_1, \dots, s_{T}]$ и ширине окна $h$ следующим образом:

$\mathbf{X} = \begin{pmatrix}
s_1 & \dots & s_{k}\\
s_2 & \dots & s_{k+1}\\
\dots & \dots & \dots\\
s_{h} & \dots & s_{T}\\
\end{pmatrix} = \big[X_1 : \dots : X_k\big], k=T-h+1, X_i = [s_i, s_{i+1}, \dots, s_{i + h - 1}] \in \mathbb{R}^h$. 

Траекторная матрица является матрицей Ганкеля, так как каждая диагональ вида $i+j=const$ содержит одинаковые элементы. Векторы $X_i$ являются точками фазовой траектории сигнала. 

В качестве альтернативных моделей для восстановления временного ряда будем использовать 1) SSA, 2) двуслойную нейросеть с ортогональными линейными преобразованиями (нелинейный PCA), 3) RNN и 4) Neural ODE. Остановимся на каждой из них подробнее.

\subsection{SSA}

В модели SSA восстановление временного ряда получается за счет разложения матрицы фазовой траектории в сумму одноранговых матриц. 

Далее вычисляются собственные значения и соответствующие собственным подпространствам ортонормированные системы векторов матрицы \[\mathbf{S}=XX^T \in \mathbb{R}^{h \times h}.\] Обозначим через \[\lambda_1 \ge \dots \ge \lambda_L \ge 0\] собственные значения $\mathbf{S}$, а через \[u_1, \dots u_h\] ортонормированную систему собственных векторов, соответствующую собственным значениям, \[v_i = \frac{\mathbf{X}^Tu_i}{\sqrt{\lambda_i}} \ (i=1, \dots, h).\] Тогда \[\mathbf{X} = \mathbf{X}_1 + \dots + \mathbf{X}_d, \mathbf{X}_i = \sqrt{\lambda_i}u_iv_i^T\] ~--- SVD-разложение $\mathbf{X}$. Далее происходит группировка матриц $\mathbf{X}_i$, их ганкелизация и восстанавливается матрица сигнала \[\hat{\mathbf{X}} = \mathbf{\tilde{X}}_{I_1} + \dots + \mathbf{\tilde{X}}_{I_m}, \ I_1 \sqcup \dots \sqcup I_m \subset \{1, \dots, h\}, \ \mathbf{\tilde{X}}_{I} = hank\big(\sum\limits_{i \in I}\mathbf{X}_i\big).\] Где $\text{hank}(X)$ ~--- ганкелизация матрицы $X$, состоящая в том, что на каждой диагонали вида $i+j=const$ все элементы заменяются на их среднее арифметическое. Параметрами модели SSA являются множества $I_1, \dots, I_m$.

\subsection{Нейросеть}
Модель двуслойной нейросети с ортогональными преобразованиями задается следующим образом: \[\mathbf{f}(\mathbf{x}) = \sigma(\mathbf{W}_1^{\mathsf{T}}\cdot \sigma(\mathbf{W}_2^{\mathsf{T}}\mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2),\] где  \[\mathbf{x} \in \mathbb{R}^{h}, \ \mathbf{W}_2 \in \mathbb{R}^{h \times d}, \mathbf{W}_1 \in \mathbb{R}^{d \times h} \ b_1 \in \mathbb{R}^h, \ b_2 \in \mathbb{R}^{h}; \mathbf{W}_1^{\mathsf{T}}\mathbf{W}_1=\mathsf{I}, \mathbf{W}_2\mathbf{W}_2^{\mathsf{T}}=\mathsf{I}.\] Последние два условия гарантируют, что преобразования будет ортогональным. Здесь нейросеть восстанавливает значения сигнала длиной в $h$ моментов времени.

\subsection{RNN}
Модель RNN задается следующим образом: \[ \mathbf{h}_t = \sigma(\mathbf{W} \cdot \mathbf{h}_{t-1} + \mathbf{V} \cdot \mathbf{x}_t),\]
\[ \mathbf{s}_{t+1} = tanh(\mathbf{w}_o^{\mathsf{T}} \cdot \mathbf{h}_t), \] где $\mathbf{h}_{t} \in \mathbb{R}^{d}$ ~--- скрытое состояние RNN в момент времени $t$, $\mathbf{W} \in \mathbb{R}^{d \times d}, \mathbf{V} \in \mathbb{R}^{d \times h}. \\
\mathbf{x}_t = [s_{t - h + 1}, \dots, s_{t}] \in \mathbb{R}^h$ - временной ряд, подающийся на вход модели, $\mathbf{w}_o \in \mathbb{R}^{d}, s_{t+1} \in \mathbb{R}$ ~--- прогноз значения сигнала в момент времени $t+1$. Параметрами модели являются матрицы $\mathbf{W}, \mathbf{V}, \mathbf{w}_0$ а также начальное скрытое состояние $\mathbf{h}_0$, которое мы зафиксируем $\mathbf{h}_0=0$.

О входном временном ряде выдвинута гипотеза о том, что точки на фазовой траектории распределены по нормальному закону, то есть что $\mathbf{x}_t = [s_t, \dots, s_{t + h - 1}] \sim \mathcal{N}(\hat{\mathbf{x}}_t, \mathbf{B})$, где $\hat{\mathbf{x}}_t$ ~--- это матожидание точки фазовой траектории в момент времени $t$, а $\mathbf{B}$ ~--- матрица ковариации. 

%ЧТО-ТО НУЖНОЕ БЫЛО!!! Мы также предполагаем, что [ссылаться на формулу, где впервые введена W]$\mathbf{w} \sim \mathcal{N}(\hat{\mathbf{w}}, \mathbf{A})$. 

В работе оцениваются матожидание вектора параметров модели $\hat{\mathbf{w}}$, а также матрица ковариации параметров $\mathbf{A}$ с помощью методов бутстрепа и вариационного вывода.

Для метрического анализа пространства параметров выбираются набор множеств индексов $\mathcal{I}_1 \sqcup \dots \sqcup \mathcal{I}_k \subset \{1, \dots, len(\mathbf{w})\}$, рассматриваются соответствующие им подвектора параметров $\mathbf{w}_{\mathcal{I}_1}, \dots, \mathbf{w}_{\mathcal{I}_k}$
Считается, что каждый $\mathbf{w}_{\mathcal{I}_j}$ соответствует <<смысловой единице>> модели. В нейросетевых моделях это строки матрицы линейного преобразования пространства параметров $\mathbf{W}$, которые обычно называют \textit{нейронами}.

Для каждого $\mathbf{w}_{\mathcal{I}}$ оценивается матожидание $\hat{\mathbf{w}}_{\mathcal{I}}$ и ковариационная матрица $\mathbf{A}_{\mathcal{I}}$, с помощью которых и проводится метрический анализ пространства параметров.

%%%[Написать что-нибудь про Neural ODE].

\section{Computational experiment}
\subsection{SSA}
Скалярный временной ряд $\mathbf{\tilde{x}} = \mathbf{x} + \varepsilon$ состоит из $n=500$ значений зашумленного синуса, измеренного в точках интервала $[-\pi, \pi]$ с равномерным шагом. Использован шум из распределения $\varepsilon \sim \mathcal{N}(0, 0.2)$. Изображение данных представлено на рис. \ref{ris:noiced_sin}.

\begin{wrapfigure}{l!}{0.42\textwidth}
\includegraphics[width=0.4\textwidth]{noiced_sin.jpg}
\caption{Зашумленный синус.}
\label{ris:noiced_sin}
\end{wrapfigure}

С помощью метода SSA построена зависимость точности восстановления временного ряда $\mathbf{x}$ из шумного ряда $\mathbf{\tilde{x}}$ в зависимости от ширины окна $h$ и количества главных компонент $k$ при восстановлении матрицы $\mathbf{X}$.

В качестве критерия качества использовались MSE и MAPE, замеренные между восстановленным рядом $\mathbf{\hat{x}}$ и истинным значением ряда $\mathbf{x}$. Результаты представлены на Рис.\ref{fig:ssa_mse_mape}.


\begin{figure}[ht]%
    \centering
    \subfloat[MSE]{\includegraphics[width=7.5cm]{jmlda-guides/SSA_MSE.jpg}}%
    %\caption{Caption}%
    \label{fig:ssa_mse}%
    \qquad
    \subfloat[MAPE]{\includegraphics[width=7.5cm]{jmlda-guides/SSA_MAPE.jpg} }%
    \caption{Метрики для различных значений $h$ и $k$. Черной линией выделена метрика между чистым и зашумленным синусом.} 
    \label{fig:ssa_mse_mape}%
\end{figure}

Различная длина графиков обусловлена тем, что длина окна $h$ не может быть больше чем количество слагаемых в SVD-восстановлении ряда. Видно, что истинное значение ряда с большой точностью восстанавливается при достатоно небольшом количестве слагаемых ($<20$), что соответствует точке минимума всех графиков на левом рисунке. Чем больше компонет используется, тем больше становится ошибка и при количестве компонент равном длине окна MSE становится таким же, как и MSE между истинным значением ряда и его шумной версией (графики приближаются к горизонтальной пунктирной линии).


Полученные результаты подтверждают практическую состоятельность SSA: алгоритм восстанавливает главный тренд при небольшом количестве сингулярных слагаемых $\mathbf{X}_i$.


\subsection{Двуслойная нейросеть с ортогональными преобразованиями}
Скалярный временной ряд $\mathbf{\tilde{x}} = \mathbf{x} + \varepsilon$ состоит из $n=500$ значений зашумленного синуса, измеренного в точках интервала $[-4\pi, 4\pi]$ с равномерным шагом. Использован шум из распределения $\varepsilon \sim \mathcal{N}(0, 0.2)$. Изображение данных представлено на рис. \ref{ris:noiced_sin_2nn}.

\begin{wrapfigure}{l!}{0.42\textwidth}
\includegraphics[width=0.4\textwidth]{noiced_sin_2nn.jpg}
\caption{Зашумленный синус.}
\label{ris:noiced_sin_2nn}
\end{wrapfigure}

С помощью нейросети решалась заадча восстановления временного ряда. Скрытая размерность нейросети $d=3$, размерность траекторного пространства $h=20$.

На случайных подвыборках обучающей выборки были обучены $N=100$ нейросетей на 25 эпохах. Оценены матожидания и матрицы ковариаций столбцов параметра $\mathbf{W}_1 \in \mathbf{R}^{3 \times 20}$. Полученные матрицы визуализированы в 3-х мерном пространстве рис~\ref{ris:2nn_total_gauss_3d_conf}.

\newpage

\begin{wrapfigure}{!ht}{0.4\textwidth}
\includegraphics[width=0.4\textwidth]{jmlda-guides/2nn_total_gauss_3d_conf.jpg}
\caption{Доверительные области 3-х мерных векторов двуслойной нейросети арихитектуры 20-3-20.}
\label{ris:2nn_total_gauss_3d_conf}
\end{wrapfigure}

Все параметры-нейроны на Рис.~\ref{ris:2nn_total_gauss_3d_conf} имеют форму вытянутых эллипсов, расположенных на удалении друг от друга. Часть эллипсов вытянута в одну и ту же сторону (уменьшения оси $OY$). В целом все эти нейроны кажутся довольно независимыми и не имеющими выраженной структуры сообществ.

В следующем эксперимента с нейросетью бралась архитектура $10-3-20$, которая означает, что на вход подаются $h_1=10$-мерные векторы, размерность скрытого пространства $d=3$, а восстановить нужно $h_1=20$ моментов времени.

На случайных подвыборках обучающей выборки были обучены $N=100$ нейросетей на 70 эпохах. Оценены матожидания и матрицы ковариаций столбцов параметра $\mathbf{W}_1 \in \mathbf{R}^{3 \times 10}$. Полученные матрицы визуализированы в 3-х мерном пространстве Рис.~\ref{ris:3d_var_10_3_20}.

\begin{wrapfigure}{!ht}{0.4\textwidth}
\includegraphics[width=0.4\textwidth]{jmlda-guides/3d_var_10_3_20.jpg}
\caption{Доверительные области 3-х мерных векторов двуслойной нейросети арихитектуры 10-3-20.}
\label{ris:3d_var_10_3_20}
\end{wrapfigure}

Здесь, в отличие от предыдущего эксперимента наблюдается разделение на 3 сообщества, соответствующие оранжевому, красному и фиолетовому эллипсоиду, которые имеют самую большую дисперсию среди нейронов (так как они большего объема).

Это частично подтверждает гипотезу о существовании структуры сообществ у нейронов.
\newpage
\section{Решение задачи}
(заимствовано из диссертации Грабового).
Решается задача байесовской дистилляции модели учителя в байесовскую версию модели ученика.

В качестве модели учителя используется модель HTNet \citep{peterson2021generalized}, которая основывается на EEGNet \citep{lawhern2018eegnet}. EEGNet является сверточной неросетью с 5 последовательными слоями: temporal convolution (различная дискретизация сигнала для получения фильтров \textit{частоты}), depthwise convolution (сверточные фильтры для получения частото-специфичных признаков), separable convolution (depthwise convolution для суммаризации информации внитри каждой частоты и затем pointwise convolution для смешение признаков).

Решается задача предсказания движения рук по электрокортикограмме головного мозга.


Задана выборка \[\mathcal{D} = \{(\mathbf{s}_i, \mathbf{y}_i)\}_{i=1}^N, \mathbf{s}_i \in \mathbb{R}^n, \mathbf{y}_i \in \mathbb{R}^m,\] где $\mathbf{s}_i$ --- $i$-ый временной ряд электрокортикограммы, $\mathbf{y}_i$ --- соответствующий временной ряд движения руки.

Задана модель учителя в виде суперпозиций линейных и нелинейных преобразований: \[ 
\mathbf{f} = \mathbf{\sigma} \circ \mathbf{U}_T \circ \mathbf{\sigma} \circ \mathbf{U}_{T-1} \circ \dots \circ \mathbf{U}_2 \circ \mathbf{\sigma} \circ \mathbf{U}_1,
\] где $T$ --- число слоев модели учителя (от $5$ до $8$ у применяемых далее моделей), $\mathbf{\sigma}$ --- функция активации, $\mathbf{U}_t$ --- матрица линейного преобразования. Матрицы $\mathbf{U}_t$, параметры модели учителя, соединяются в вектор параметров $\mathbf{u}$ модели учителя $\mathbf{f}:$ 
\[
\mathbf{u} = vec([\mathbf{U}_T, \mathbf{U}_{T-1}, \dots, \mathbf{U}_{1}]).
\]

Мы работаем в предположении, что параметры учителя распределены нормально.
Для модели учителя оценивается апостериорное распределение вектора параметров \[p(\mathbf{u}) = p(\mathcal{N}(\mathbf{\overline{u}}_{ps}, \mathbf{A}_{ps}^{-1})).\]
В качестве модели ученика используется байесовская нейронная сеть [вставить ссылку на статью], задаваемая аналогично модели учителя:
\[
\mathbf{g} = \mathbf{\sigma} \circ \mathbf{W}_L \circ \mathbf{\sigma} \circ \mathbf{W}_{L-1} \circ \dots \circ \mathbf{W}_2 \circ \mathbf{\sigma} \circ \mathbf{W}_1.
\]
Мы считаем, что \[\mathbf{w} \sim \mathbf{N}(\mathbf{\overline{w}}_t, \mathbf{B}^{-1}).\] 

Алгоритм римановской дистилляции представлен ниже.

\begin{algorithm}[H] 
         \caption{Алгоритм Римановской дистилляции}\label{alg:riman_distil} 
         \begin{enumerate}
             \item Зафиксировать структуру модели ученика $\mathbf{g}$ и учителя $\mathbf{g}$, то есть параметры $T, L$ и размеры матриц $\mathbf{U}_t, \mathbf{W}_t$.
             \item Обучить модель учителя $\mathbf{g}$.
            \item Оценить апостериорное распределение параметров учителя $p_{ps}(\mathbf{u})$, то есть параметры $\mathbf{\overline{u}}, \mathbf{A}^{-1}$.
            \item Назначить параметрам ученика априорное распределение $p(\mathbf{w}) \sim p_{ps}(\mathbf{u})$. Где $\sim$ обозначает преобразование распределения, описанное в работе \citep{grabovoy2021bayesian}.
            \item Обучить байесовскую сеть ученика на ответах учителя, получив апостериорное распределение парметров ученика $p(\mathbf{w}|\mathcal{D}) = p(\mathbf{N}(\mathbf{\overline{w}}_t, \mathbf{B}^{-1}))$.
            \item Провести метричекий анализ пространства параметров ученика и снизить его размерность.
         \end{enumerate}
 \end{algorithm}

Обучение байесовской модели ученика производится при прмощи вариационного вывода на основе совместного правдоподобия данных:
\[
\mathcal{L}(\mathcal{D}) = \log p(\mathcal{D}) = \log \int \limits_{\mathbf{w} \in \mathbb{R}^{k}}p(\mathcal{D}|\mathbf{w}) \cdot p(\mathbf{w}) d\mathbf{w},
\] где $p(\mathbf{w})$ - априорное распределение параметров модели ученика, которое получается преобразованием из апостериорного распределения параметров учителя. С учетом нашего предположения, что оно является нормальным, обучение ученика сводится к решению задачи:
\[
\mathbf{\widehat{w}} = \argmin \limits_{\mathbf{\mu}, \mathbf{\Sigma}, \mathbf{w}} \mathsf{D}_{\mathsf{KL}}(p(\mathbf{w}|\mathcal{D})||p(\mathbf{w})) - \log p(\mathbf{y}|\mathbf{X}, \mathbf{w}).
\] Где второе слагаемое - логарифм правдоподобия выборки, а первое - $\mathsf{KL}$-дивергенция между априорным и апостериорным распределением параметров ученика.


\subsection{Вычисление ковариации}
Вычисление ковариации будет производится следующими способами, представленными в статье \citep{chen2020statistical}.

Пусть $\mathbf{w}^* \in \mathbb{R}^d$ - истинный вектор параметров модели, то есть
$$
\mathbf{w}^* = \arg \min\limits_{\mathbf{w} \in \mathbb{W}} F(\mathbf{w})
$$
$$
F(\mathbf{w}) = \mathsf{E}_{\mathbf{\xi} \sim \mathcal{D}}S(\mathbf{w}, \mathbf{\xi})
$$

Где $\mathbf{\xi}$ - элемент, сэмплированный из обучающий выборки, $S$ - функция ошибки.

При оптимизации параметров модели методом SGD с начальной точкой $\mathbf{w}_0$ происходит итеративный процесс:
$$
\mathbf{w}_i = \mathbf{w}_{i-1} - \nu_i\nabla S(\mathbf{w}_{i-1}, \mathbf{\xi}_i)
$$
Где $\mathbf{\xi}_i$ сэмплировано из обучающей выборки $\mathcal{D}$, $\nabla S(\mathbf{w}_{i-1}, \mathbf{\xi}_i)$ - градиент функции ошибки относительно весов модели. 

В версии ASGS (averaged SGD), которая будет использоваться далее, в качестве ответа возвращается $\overline{\mathbf{w}_n} = \frac{1}{n}\sum\limits_{i=1}^n\mathbf{w}_i$.

Обозначим $A=\nabla^2F(\mathbf{w}^*)$ - Гессиан матожидания ошибки, $S=\mathsf{E\big[\nabla S(\mathbf{w}^*, \xi) \cdot \nabla S(\mathbf{w}^*, \xi)^{\mathsf{T}}\big]}$ - ковариационная матрица $\nabla S(\mathbf{w}^*, \xi)$.

В статье упоминается, что при условиях на выпуклость $F$ верно следующее асимптотическое равенство:
$$
\sqrt{n}(\overline{\mathbf{w}}_n - \mathbf{w}^*) \to \mathcal{N}(0, A^{-1}SA^{-1})
$$

Таким образом, состоятельная оценка матрицы ковариации $\sqrt{n}\overline{\mathbf{w}}_n$ это $A^{-1}SA^{-1}$.

\begin{algorithm}[H] 
         \caption{Оффлайн метод оценки ковариационной матрицы параметров}\label{alg:offline_cov} 
         \begin{enumerate}
             \item Обучить модель, получив приближение $\mathbf{w}^*$.
             \item Приблизить матрицы $A, S$ с помощью сэмплирования:
             $$
             A_n = \frac{1}{n}\sum\limits_{i=1}^n\nabla^2S(\mathbf{w}^*, \xi_i), \ 
             S_n = \frac{1}{n}\sum\limits_{i=1}^n\nabla S(\mathbf{w}^*, \xi_i) \cdot S(\mathbf{w}^*, \xi_i)^{\mathsf{T}}
             $$
             \item Оценить ковариационную матрицу $cov(\sqrt{n}\overline{\mathbf{w}}_n) \approx A_n^{-1}S_nA_n^{-1}$.
         \end{enumerate}
 \end{algorithm}

 \begin{algorithm}[H] 
         \caption{Онлайн метод оценки ковариационной матрицы параметров}\label{alg:online_cov} 
         В процессе обучения модели с помощью SGD:
         \begin{enumerate}
             \item Приблизить матрицы $A, S$ с помощью $n$ слагаемых:
             $$
             A_n = \frac{1}{n}\sum\limits_{i=1}^n\nabla^2S(\mathbf{w}^*, \xi_i), \ 
             S_n = \frac{1}{n}\sum\limits_{i=1}^n\nabla S(\mathbf{w}^*, \xi_i) \cdot S(\mathbf{w}^*, \xi_i)^{\mathsf{T}}
             $$
         \end{enumerate}
         Примечание: здесь $\xi_i$ - не сэмплируются, а берутся из алгоритма оптимизации во время SGD.
 \end{algorithm}

 \subsection{Вычислительный эксперимент} 

\subsection{Оценка ковариаций}
 Решается задача восстановления временного ряда Accelerometer Motion Sense с помощью 2-х слойных нейросетей.

 Модели 2-х слойной нейросети архитектуры $100-3-100$ в течение 50 эпох обучены на датасете Accelerometer Motion Sense, состоящем из $10000$ замеров ускорения устройства. После обучения матрицы ковариаций нейронов модели вычислялись с помощью техники Bootstrap, по алгоритму \ref{alg:offline_cov} (техника Hessian), с помощью обучения байесовской нейросети (техника Bayes). Результаты представлены на Рис.\ref{fg:neurons_3_methods}.


\begin{figure}[!ht]
  \subfloat[Bootstrap]{\includegraphics[width=0.35\textwidth]{jmlda-guides/covs_bootstrap.jpg}}
  \subfloat[Hessian]{\includegraphics[width=0.35\textwidth]{jmlda-guides/covs_100-3-100-hessian.jpg}}
  \subfloat[Bayes]{\includegraphics[width=0.35\textwidth]{jmlda-guides/covs_100-3-100-bayes.jpg}}
\caption{Первые 16 нейронов нейросетей архитектуры $100-3-100$, обученных на задаче восстановления временного ряда Motion Sense.}
\label{fg:neurons_3_methods}
\end{figure}

Структура нейронов одинакова у всех $3$ методов: они все расположены <<по кругу>>. С учетом того, что период временного ряда - $100$ измерений, то очевидно, что размер выходного слоя нейросети можно уменьшить до $16$. Значительно различается масштаб: оценки методом Bootstrap имеют наименьшие дисперсии и разброс нейронов, а по методу Hessian - наибольшие. Также видно, что метода Hessian и Bayes дает плоские нейроны (одно из собственных значений ковариационной матрицы во много раз меньше $2$ других), в отличие от метода Bootstrap. Это дает потенциальную возможность произвести линейные преобразования нейронов скрытого слоя, чтобы сократить размер скрытого пространства с $3$ до $2$ и представляет интерес для дальнейших исследований.


\subsection{Снижение размерности с помощью кластеризации.}
На 3 датасетах: MNIST, Iris flower и Motion Sence решались задачи классификации и регрессии с помощью полносвязных нейронных сетей. Архитектуры: $784-8-100-10$, $4-128-3$ и $100-10-100$ соответственно. 

Оценки ковариаций и матожиданий нейронов проведены с помощью техники bootstrap: были натренированы $100$ моделей с помощью $SGD$ и подсчитаны несмещенные оценки ковариаций и дисперсий нейронов по формулам:

$$
\mathsf{E}[\mathbf{w}] = \frac{1}{n}\sum\limits_{t=1}^n\mathbf{w}^t, \ \ 
cov(\mathbf{w}_i, \mathbf{w}_j) = \frac{\sum\limits_{t=1}^n(\mathbf{w}_i^t - \overline{\mathbf{w}_i})(\mathbf{w}_j^t - \overline{\mathbf{w}_j})}{n-1}
$$

Кластеризация нейронов проводилась с помощью алгоритма $k-means$ двумя способами: в первом расстояние между нейронами считалось, как евклидово расстояние между их средними, а во втором как расстояние Вассерштайна между двумя нормальными распределениями:

$$
\rho_1(\mathbf{w_1}, \mathbf{w_2}) = ||\mathbf{m}_1 - \mathbf{m}_2||_2^2$$
$$
\rho_2(\mathbf{w_1}, \mathbf{w_2}) = ||\mathbf{m}_1 - \mathbf{m}_2||_2^2 + Tr(\mathbf{\Sigma}_1 + \mathbf{\Sigma}_2 - 2(\mathbf{\Sigma}_1^{0.5}\mathbf{\Sigma}_2\mathbf{\Sigma}_1^{0.5})^{0.5})
$$

После кластеризации каждый нейрон заменялся на центроид своего кластера и производились замеры качества: MSE-ошибка для задачи регрессии и Accuracy для классификации.

Результаты экспериментов представлены на Рис.\ref{fg:dim_red_boot}.
\begin{figure}[!ht]
  \subfloat[MotionSense. 10-и мерные нейроны.]{\includegraphics[width=0.35\textwidth]{jmlda-guides/MotionSence_bootstrap.jpg}}
  \subfloat[MNIST. 8-ми мерные нейроны.]{\includegraphics[width=0.35\textwidth]{jmlda-guides/MNIST_bootstrap.jpg}}
  \subfloat[IRIS. 4-х мерные нейроны.]{\includegraphics[width=0.35\textwidth]{jmlda-guides/Iris_bootstrap.jpg}}
\caption{Зависимость точности модели от сложности на 3 датасетах. Синяя кривая соответствует кластеризации по мерике Вассерштайна, а красная --- по евклидовой метрике.}
\label{fg:dim_red_boot}
\end{figure}

На графиках показана зависимость точности модели от ее сложности. Чем большее число кластеров выделено, тем больше параметров имеет модель и тем она сложнее и, соответственно, растет качество предсказаний. Видим, что оба способа кластеризации показывают одинаковые результаты на первых двух датасетах, а на датасете IRIS кластеризация Вассерштайна показала лучше. Скорее всего это связано с размерностью нейронов: чем она меньше, тем более точно можно оценить ковариационную матрицу и тем лучше получающиеся кластеры.

\section{Заключение}
В работе исследовано, как выглядит пространство нейронов нейросетей при задаче восстановления временного ряда. На временном ряде MotionSence показано, что $3$-х мерные  нейроны имеют регулярную структуру и располагаются <<по кругу>> с периодом $16$, что в 5 раз меньше периода самого временного ряда, что представляет возможность уменьшить размер скрытого пространства сети более чем в $5$ раз.


Также в работе исследована возможность снижения размерности пространства параметров нейросетей с помощью кластеризации нейронов на 3 датасетах разной природы: MotionSence - скалярный временной ряд, решалась задача регрессии; IRIS - табличные данные, решалась задача классификации; MNIST - картинки, решалась задача классификации. 

Показано, что на датасете IRIS можно удалить до $50\%$ нейронов, обеспечивая в то же время более чем $90\%$ точность классификации. На остальных датасетах эффект не такой значительный.


Будущие исследования будут посвящены изучению сжатия больших, промышленных нейросетей и особое внимание будет уделено сжатию сетей, восстанавливающих временные ряды, как потенциально имеющих избыточное количество параметров.

Также будут разработаны новые методы оценки матриц ковариации параметров: с помощью низкоранговых байесовских сетей, полубайесовских и блочно-диагонально-байесовских.


\begin{comment}


Отметим, что матрицы $A_n, S_n$ вычислялись \textbf{отдельно} для каждого нейрона, так как мы работаем в предположении, что производная веса нейрона по весам других нейронов $=0$, что делает матрицы $A_n, S_n$ блочно-диагональными и дает возможность эффективно вычислять обратные матрицы.

Р

 
Эксперимент по восстановалнию временного ряда Accelerometer Motion Sense произведен 


Шаг 1: мы решаем задачу оптимизации каждого слоя с помощью ковариационных матриц.

Шаг 2: (для оптимизации)в течение процедуры оптимизации используем  такую функцию ошибки, которая вклячает часть, называющуюся дистилляцией.

Есть 2 модели.
1 сложная - HTNet, 
2 простая - наша - байесовская модель.

Что мы делаем:

2 раза пишем формулу Байеса. Считаем правдоподобие сложной модели, постериор сложной модели по весам - это prior по весам простой модели.

Мы его фиксируем согласно статьям Грабового (2 последние статьи), [диссертация Грабового]

И оптимизируем posterior простой модели с помощью простого механизма. [читать статью про "Функция ошибки в задачах линейной регрессии"].

ЧТОБЫ СДЕЛАТЬ маленькую модель, нужно анализировать ков матрицы! маленькой модели.
Когда настраиваем маленькую модель мы одновременно настраиваем веса (матож) и (cov). А то, как мы выбираем структуру модели - с помощью анализа cov матриц - (последний пункт нашей постановки) - метод Белсли (Бахтеев + Грабовой - мпнс "метод белсли в выборе нейросетей"). Берем принцыпы Белсли + Байес = статья! 

Мы фиксируем структуру хорошей переобученной модели. 

<--- Это все диссер Грабового (о том, как апостериорное распределение модели учителя преобразовать в априорное на модели ученика с другой арзитектурой.


\end{comment}


\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .





\end{document}
